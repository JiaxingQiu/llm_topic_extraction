- lmsys/vicuna-13b-v1.5
    + Vicuna is a chat assistant trained by fine-tuning Llama 2 on user-shared conversations collected from ShareGPT.
    + Llama 2's training corpus is sourced from publicly available and licensed datasets, emphasizing open and legally accessible material.
    + This includes books, academic papers, websites, and other forms of digital text available for research and non-commercial use.
- Mistral-7B-Instruct-v0.3 
    + a diverse set of publicly available instruction-following datasets hosted on Hugging Face
- Qwen/Qwen2.5-7B-Instruct
    + Qwen2.5 models were pre-trained on a large-scale dataset comprising up to 18 trillion tokens. This dataset includes diverse sources such as web texts, code snippets, encyclopedias, exam questions, and synthetic data generated by earlier Qwen models.
- Llama 3.1
    + developed by Meta, was trained on an extensive and diverse dataset comprising approximately 15 trillion tokens. This dataset was meticulously curated from various publicly available sources to enhance the model's performance across multiple languages and tasks.
    + Web Pages, Books, Scientific Articles, Multilingual Texts, Code Repositories