{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yg9bq/miniconda/ENTER/envs/llama/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "# Clear GPU cache\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "model_name = \"meta-llama/Llama-3.1-70B-Instruct\"  # Replace with exact path for 8B model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 30/30 [03:53<00:00,  7.78s/it]\n",
      "Some parameters are on the meta device because they were offloaded to the cpu and disk.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "You cannot use both `pipeline(... torch_dtype=..., model_kwargs={\"torch_dtype\":...})` as those arguments might conflict, use only one.)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 15\u001b[0m\n\u001b[1;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m      6\u001b[0m     model_name,                   \n\u001b[1;32m      7\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m     trust_remote_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     12\u001b[0m )\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Initialize the text generation pipeline\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m pipe \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext-generation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtorch_dtype\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbfloat16\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquantization_config\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mload_in_4bit\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m}\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat16\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Explicitly set device to GPU if available, else CPU\u001b[39;49;00m\n\u001b[1;32m     25\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# # Example usage of the pipeline\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# output = pipe(\"Once upon a time,\", max_length=50)\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# print(output)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda/ENTER/envs/llama/lib/python3.10/site-packages/transformers/pipelines/__init__.py:882\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m    880\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch_dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch_dtype\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m model_kwargs:\n\u001b[0;32m--> 882\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    883\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYou cannot use both `pipeline(... torch_dtype=..., model_kwargs=\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch_dtype\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m:...})` as those\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    884\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m arguments might conflict, use only one.)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    885\u001b[0m         )\n\u001b[1;32m    886\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(torch_dtype, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch, torch_dtype):\n\u001b[1;32m    887\u001b[0m         torch_dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(torch, torch_dtype)\n",
      "\u001b[0;31mValueError\u001b[0m: You cannot use both `pipeline(... torch_dtype=..., model_kwargs={\"torch_dtype\":...})` as those arguments might conflict, use only one.)"
     ]
    }
   ],
   "source": [
    "# Check if CUDA is available\n",
    "device = 0 if torch.cuda.is_available() else -1  # Use GPU if available, otherwise fallback to CPU\n",
    "\n",
    "# Load the model with the appropriate device settings\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,                   \n",
    "    return_dict=True,\n",
    "    low_cpu_mem_usage=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\" if device == 0 else None,  # Automatically distribute across devices or use CPU\n",
    "    trust_remote_code=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the text generation pipeline\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    model_kwargs={\n",
    "        \"torch_dtype\": torch.bfloat16,\n",
    "        \"quantization_config\": {\"load_in_4bit\": True}\n",
    "    }, \n",
    "    device_map=\"auto\" if device == 0 else None,  # Explicitly set device to GPU if available, else CPU\n",
    ")\n",
    "\n",
    "# # Example usage of the pipeline\n",
    "# output = pipe(\"Once upon a time,\", max_length=50)\n",
    "# print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.embed_tokens.weight is on cuda:0\n",
      "model.layers.0.self_attn.q_proj.weight is on cuda:0\n",
      "model.layers.0.self_attn.k_proj.weight is on cuda:0\n",
      "model.layers.0.self_attn.v_proj.weight is on cuda:0\n",
      "model.layers.0.self_attn.o_proj.weight is on cuda:0\n",
      "model.layers.0.mlp.gate_proj.weight is on cuda:0\n",
      "model.layers.0.mlp.up_proj.weight is on cuda:0\n",
      "model.layers.0.mlp.down_proj.weight is on cuda:0\n",
      "model.layers.0.input_layernorm.weight is on cuda:0\n",
      "model.layers.0.post_attention_layernorm.weight is on cuda:0\n",
      "model.layers.1.self_attn.q_proj.weight is on cuda:0\n",
      "model.layers.1.self_attn.k_proj.weight is on cuda:0\n",
      "model.layers.1.self_attn.v_proj.weight is on cuda:0\n",
      "model.layers.1.self_attn.o_proj.weight is on cuda:0\n",
      "model.layers.1.mlp.gate_proj.weight is on cuda:0\n",
      "model.layers.1.mlp.up_proj.weight is on cuda:0\n",
      "model.layers.1.mlp.down_proj.weight is on cuda:0\n",
      "model.layers.1.input_layernorm.weight is on cuda:0\n",
      "model.layers.1.post_attention_layernorm.weight is on cuda:0\n",
      "model.layers.2.self_attn.q_proj.weight is on cuda:0\n",
      "model.layers.2.self_attn.k_proj.weight is on cuda:0\n",
      "model.layers.2.self_attn.v_proj.weight is on cuda:0\n",
      "model.layers.2.self_attn.o_proj.weight is on cuda:0\n",
      "model.layers.2.mlp.gate_proj.weight is on cuda:0\n",
      "model.layers.2.mlp.up_proj.weight is on cuda:0\n",
      "model.layers.2.mlp.down_proj.weight is on cuda:0\n",
      "model.layers.2.input_layernorm.weight is on cuda:0\n",
      "model.layers.2.post_attention_layernorm.weight is on cuda:0\n",
      "model.layers.3.self_attn.q_proj.weight is on cuda:1\n",
      "model.layers.3.self_attn.k_proj.weight is on cuda:1\n",
      "model.layers.3.self_attn.v_proj.weight is on cuda:1\n",
      "model.layers.3.self_attn.o_proj.weight is on cuda:1\n",
      "model.layers.3.mlp.gate_proj.weight is on cuda:1\n",
      "model.layers.3.mlp.up_proj.weight is on cuda:1\n",
      "model.layers.3.mlp.down_proj.weight is on cuda:1\n",
      "model.layers.3.input_layernorm.weight is on cuda:1\n",
      "model.layers.3.post_attention_layernorm.weight is on cuda:1\n",
      "model.layers.4.self_attn.q_proj.weight is on cuda:1\n",
      "model.layers.4.self_attn.k_proj.weight is on cuda:1\n",
      "model.layers.4.self_attn.v_proj.weight is on cuda:1\n",
      "model.layers.4.self_attn.o_proj.weight is on cuda:1\n",
      "model.layers.4.mlp.gate_proj.weight is on cuda:1\n",
      "model.layers.4.mlp.up_proj.weight is on cuda:1\n",
      "model.layers.4.mlp.down_proj.weight is on cuda:1\n",
      "model.layers.4.input_layernorm.weight is on cuda:1\n",
      "model.layers.4.post_attention_layernorm.weight is on cuda:1\n",
      "model.layers.5.self_attn.q_proj.weight is on cuda:1\n",
      "model.layers.5.self_attn.k_proj.weight is on cuda:1\n",
      "model.layers.5.self_attn.v_proj.weight is on cuda:1\n",
      "model.layers.5.self_attn.o_proj.weight is on cuda:1\n",
      "model.layers.5.mlp.gate_proj.weight is on cuda:1\n",
      "model.layers.5.mlp.up_proj.weight is on cuda:1\n",
      "model.layers.5.mlp.down_proj.weight is on cuda:1\n",
      "model.layers.5.input_layernorm.weight is on cuda:1\n",
      "model.layers.5.post_attention_layernorm.weight is on cuda:1\n",
      "model.layers.6.self_attn.q_proj.weight is on cuda:1\n",
      "model.layers.6.self_attn.k_proj.weight is on cuda:1\n",
      "model.layers.6.self_attn.v_proj.weight is on cuda:1\n",
      "model.layers.6.self_attn.o_proj.weight is on cuda:1\n",
      "model.layers.6.mlp.gate_proj.weight is on cuda:1\n",
      "model.layers.6.mlp.up_proj.weight is on cuda:1\n",
      "model.layers.6.mlp.down_proj.weight is on cuda:1\n",
      "model.layers.6.input_layernorm.weight is on cuda:1\n",
      "model.layers.6.post_attention_layernorm.weight is on cuda:1\n",
      "model.layers.7.self_attn.q_proj.weight is on cuda:1\n",
      "model.layers.7.self_attn.k_proj.weight is on cuda:1\n",
      "model.layers.7.self_attn.v_proj.weight is on cuda:1\n",
      "model.layers.7.self_attn.o_proj.weight is on cuda:1\n",
      "model.layers.7.mlp.gate_proj.weight is on cuda:1\n",
      "model.layers.7.mlp.up_proj.weight is on cuda:1\n",
      "model.layers.7.mlp.down_proj.weight is on cuda:1\n",
      "model.layers.7.input_layernorm.weight is on cuda:1\n",
      "model.layers.7.post_attention_layernorm.weight is on cuda:1\n",
      "model.layers.8.self_attn.q_proj.weight is on cuda:1\n",
      "model.layers.8.self_attn.k_proj.weight is on cuda:1\n",
      "model.layers.8.self_attn.v_proj.weight is on cuda:1\n",
      "model.layers.8.self_attn.o_proj.weight is on cuda:1\n",
      "model.layers.8.mlp.gate_proj.weight is on cuda:1\n",
      "model.layers.8.mlp.up_proj.weight is on cuda:1\n",
      "model.layers.8.mlp.down_proj.weight is on cuda:1\n",
      "model.layers.8.input_layernorm.weight is on cuda:1\n",
      "model.layers.8.post_attention_layernorm.weight is on cuda:1\n",
      "model.layers.9.self_attn.q_proj.weight is on meta\n",
      "model.layers.9.self_attn.k_proj.weight is on meta\n",
      "model.layers.9.self_attn.v_proj.weight is on meta\n",
      "model.layers.9.self_attn.o_proj.weight is on meta\n",
      "model.layers.9.mlp.gate_proj.weight is on meta\n",
      "model.layers.9.mlp.up_proj.weight is on meta\n",
      "model.layers.9.mlp.down_proj.weight is on meta\n",
      "model.layers.9.input_layernorm.weight is on meta\n",
      "model.layers.9.post_attention_layernorm.weight is on meta\n",
      "model.layers.10.self_attn.q_proj.weight is on meta\n",
      "model.layers.10.self_attn.k_proj.weight is on meta\n",
      "model.layers.10.self_attn.v_proj.weight is on meta\n",
      "model.layers.10.self_attn.o_proj.weight is on meta\n",
      "model.layers.10.mlp.gate_proj.weight is on meta\n",
      "model.layers.10.mlp.up_proj.weight is on meta\n",
      "model.layers.10.mlp.down_proj.weight is on meta\n",
      "model.layers.10.input_layernorm.weight is on meta\n",
      "model.layers.10.post_attention_layernorm.weight is on meta\n",
      "model.layers.11.self_attn.q_proj.weight is on meta\n",
      "model.layers.11.self_attn.k_proj.weight is on meta\n",
      "model.layers.11.self_attn.v_proj.weight is on meta\n",
      "model.layers.11.self_attn.o_proj.weight is on meta\n",
      "model.layers.11.mlp.gate_proj.weight is on meta\n",
      "model.layers.11.mlp.up_proj.weight is on meta\n",
      "model.layers.11.mlp.down_proj.weight is on meta\n",
      "model.layers.11.input_layernorm.weight is on meta\n",
      "model.layers.11.post_attention_layernorm.weight is on meta\n",
      "model.layers.12.self_attn.q_proj.weight is on meta\n",
      "model.layers.12.self_attn.k_proj.weight is on meta\n",
      "model.layers.12.self_attn.v_proj.weight is on meta\n",
      "model.layers.12.self_attn.o_proj.weight is on meta\n",
      "model.layers.12.mlp.gate_proj.weight is on meta\n",
      "model.layers.12.mlp.up_proj.weight is on meta\n",
      "model.layers.12.mlp.down_proj.weight is on meta\n",
      "model.layers.12.input_layernorm.weight is on meta\n",
      "model.layers.12.post_attention_layernorm.weight is on meta\n",
      "model.layers.13.self_attn.q_proj.weight is on meta\n",
      "model.layers.13.self_attn.k_proj.weight is on meta\n",
      "model.layers.13.self_attn.v_proj.weight is on meta\n",
      "model.layers.13.self_attn.o_proj.weight is on meta\n",
      "model.layers.13.mlp.gate_proj.weight is on meta\n",
      "model.layers.13.mlp.up_proj.weight is on meta\n",
      "model.layers.13.mlp.down_proj.weight is on meta\n",
      "model.layers.13.input_layernorm.weight is on meta\n",
      "model.layers.13.post_attention_layernorm.weight is on meta\n",
      "model.layers.14.self_attn.q_proj.weight is on meta\n",
      "model.layers.14.self_attn.k_proj.weight is on meta\n",
      "model.layers.14.self_attn.v_proj.weight is on meta\n",
      "model.layers.14.self_attn.o_proj.weight is on meta\n",
      "model.layers.14.mlp.gate_proj.weight is on meta\n",
      "model.layers.14.mlp.up_proj.weight is on meta\n",
      "model.layers.14.mlp.down_proj.weight is on meta\n",
      "model.layers.14.input_layernorm.weight is on meta\n",
      "model.layers.14.post_attention_layernorm.weight is on meta\n",
      "model.layers.15.self_attn.q_proj.weight is on meta\n",
      "model.layers.15.self_attn.k_proj.weight is on meta\n",
      "model.layers.15.self_attn.v_proj.weight is on meta\n",
      "model.layers.15.self_attn.o_proj.weight is on meta\n",
      "model.layers.15.mlp.gate_proj.weight is on meta\n",
      "model.layers.15.mlp.up_proj.weight is on meta\n",
      "model.layers.15.mlp.down_proj.weight is on meta\n",
      "model.layers.15.input_layernorm.weight is on meta\n",
      "model.layers.15.post_attention_layernorm.weight is on meta\n",
      "model.layers.16.self_attn.q_proj.weight is on meta\n",
      "model.layers.16.self_attn.k_proj.weight is on meta\n",
      "model.layers.16.self_attn.v_proj.weight is on meta\n",
      "model.layers.16.self_attn.o_proj.weight is on meta\n",
      "model.layers.16.mlp.gate_proj.weight is on meta\n",
      "model.layers.16.mlp.up_proj.weight is on meta\n",
      "model.layers.16.mlp.down_proj.weight is on meta\n",
      "model.layers.16.input_layernorm.weight is on meta\n",
      "model.layers.16.post_attention_layernorm.weight is on meta\n",
      "model.layers.17.self_attn.q_proj.weight is on meta\n",
      "model.layers.17.self_attn.k_proj.weight is on meta\n",
      "model.layers.17.self_attn.v_proj.weight is on meta\n",
      "model.layers.17.self_attn.o_proj.weight is on meta\n",
      "model.layers.17.mlp.gate_proj.weight is on meta\n",
      "model.layers.17.mlp.up_proj.weight is on meta\n",
      "model.layers.17.mlp.down_proj.weight is on meta\n",
      "model.layers.17.input_layernorm.weight is on meta\n",
      "model.layers.17.post_attention_layernorm.weight is on meta\n",
      "model.layers.18.self_attn.q_proj.weight is on meta\n",
      "model.layers.18.self_attn.k_proj.weight is on meta\n",
      "model.layers.18.self_attn.v_proj.weight is on meta\n",
      "model.layers.18.self_attn.o_proj.weight is on meta\n",
      "model.layers.18.mlp.gate_proj.weight is on meta\n",
      "model.layers.18.mlp.up_proj.weight is on meta\n",
      "model.layers.18.mlp.down_proj.weight is on meta\n",
      "model.layers.18.input_layernorm.weight is on meta\n",
      "model.layers.18.post_attention_layernorm.weight is on meta\n",
      "model.layers.19.self_attn.q_proj.weight is on meta\n",
      "model.layers.19.self_attn.k_proj.weight is on meta\n",
      "model.layers.19.self_attn.v_proj.weight is on meta\n",
      "model.layers.19.self_attn.o_proj.weight is on meta\n",
      "model.layers.19.mlp.gate_proj.weight is on meta\n",
      "model.layers.19.mlp.up_proj.weight is on meta\n",
      "model.layers.19.mlp.down_proj.weight is on meta\n",
      "model.layers.19.input_layernorm.weight is on meta\n",
      "model.layers.19.post_attention_layernorm.weight is on meta\n",
      "model.layers.20.self_attn.q_proj.weight is on meta\n",
      "model.layers.20.self_attn.k_proj.weight is on meta\n",
      "model.layers.20.self_attn.v_proj.weight is on meta\n",
      "model.layers.20.self_attn.o_proj.weight is on meta\n",
      "model.layers.20.mlp.gate_proj.weight is on meta\n",
      "model.layers.20.mlp.up_proj.weight is on meta\n",
      "model.layers.20.mlp.down_proj.weight is on meta\n",
      "model.layers.20.input_layernorm.weight is on meta\n",
      "model.layers.20.post_attention_layernorm.weight is on meta\n",
      "model.layers.21.self_attn.q_proj.weight is on meta\n",
      "model.layers.21.self_attn.k_proj.weight is on meta\n",
      "model.layers.21.self_attn.v_proj.weight is on meta\n",
      "model.layers.21.self_attn.o_proj.weight is on meta\n",
      "model.layers.21.mlp.gate_proj.weight is on meta\n",
      "model.layers.21.mlp.up_proj.weight is on meta\n",
      "model.layers.21.mlp.down_proj.weight is on meta\n",
      "model.layers.21.input_layernorm.weight is on meta\n",
      "model.layers.21.post_attention_layernorm.weight is on meta\n",
      "model.layers.22.self_attn.q_proj.weight is on meta\n",
      "model.layers.22.self_attn.k_proj.weight is on meta\n",
      "model.layers.22.self_attn.v_proj.weight is on meta\n",
      "model.layers.22.self_attn.o_proj.weight is on meta\n",
      "model.layers.22.mlp.gate_proj.weight is on meta\n",
      "model.layers.22.mlp.up_proj.weight is on meta\n",
      "model.layers.22.mlp.down_proj.weight is on meta\n",
      "model.layers.22.input_layernorm.weight is on meta\n",
      "model.layers.22.post_attention_layernorm.weight is on meta\n",
      "model.layers.23.self_attn.q_proj.weight is on meta\n",
      "model.layers.23.self_attn.k_proj.weight is on meta\n",
      "model.layers.23.self_attn.v_proj.weight is on meta\n",
      "model.layers.23.self_attn.o_proj.weight is on meta\n",
      "model.layers.23.mlp.gate_proj.weight is on meta\n",
      "model.layers.23.mlp.up_proj.weight is on meta\n",
      "model.layers.23.mlp.down_proj.weight is on meta\n",
      "model.layers.23.input_layernorm.weight is on meta\n",
      "model.layers.23.post_attention_layernorm.weight is on meta\n",
      "model.layers.24.self_attn.q_proj.weight is on meta\n",
      "model.layers.24.self_attn.k_proj.weight is on meta\n",
      "model.layers.24.self_attn.v_proj.weight is on meta\n",
      "model.layers.24.self_attn.o_proj.weight is on meta\n",
      "model.layers.24.mlp.gate_proj.weight is on meta\n",
      "model.layers.24.mlp.up_proj.weight is on meta\n",
      "model.layers.24.mlp.down_proj.weight is on meta\n",
      "model.layers.24.input_layernorm.weight is on meta\n",
      "model.layers.24.post_attention_layernorm.weight is on meta\n",
      "model.layers.25.self_attn.q_proj.weight is on meta\n",
      "model.layers.25.self_attn.k_proj.weight is on meta\n",
      "model.layers.25.self_attn.v_proj.weight is on meta\n",
      "model.layers.25.self_attn.o_proj.weight is on meta\n",
      "model.layers.25.mlp.gate_proj.weight is on meta\n",
      "model.layers.25.mlp.up_proj.weight is on meta\n",
      "model.layers.25.mlp.down_proj.weight is on meta\n",
      "model.layers.25.input_layernorm.weight is on meta\n",
      "model.layers.25.post_attention_layernorm.weight is on meta\n",
      "model.layers.26.self_attn.q_proj.weight is on meta\n",
      "model.layers.26.self_attn.k_proj.weight is on meta\n",
      "model.layers.26.self_attn.v_proj.weight is on meta\n",
      "model.layers.26.self_attn.o_proj.weight is on meta\n",
      "model.layers.26.mlp.gate_proj.weight is on meta\n",
      "model.layers.26.mlp.up_proj.weight is on meta\n",
      "model.layers.26.mlp.down_proj.weight is on meta\n",
      "model.layers.26.input_layernorm.weight is on meta\n",
      "model.layers.26.post_attention_layernorm.weight is on meta\n",
      "model.layers.27.self_attn.q_proj.weight is on meta\n",
      "model.layers.27.self_attn.k_proj.weight is on meta\n",
      "model.layers.27.self_attn.v_proj.weight is on meta\n",
      "model.layers.27.self_attn.o_proj.weight is on meta\n",
      "model.layers.27.mlp.gate_proj.weight is on meta\n",
      "model.layers.27.mlp.up_proj.weight is on meta\n",
      "model.layers.27.mlp.down_proj.weight is on meta\n",
      "model.layers.27.input_layernorm.weight is on meta\n",
      "model.layers.27.post_attention_layernorm.weight is on meta\n",
      "model.layers.28.self_attn.q_proj.weight is on meta\n",
      "model.layers.28.self_attn.k_proj.weight is on meta\n",
      "model.layers.28.self_attn.v_proj.weight is on meta\n",
      "model.layers.28.self_attn.o_proj.weight is on meta\n",
      "model.layers.28.mlp.gate_proj.weight is on meta\n",
      "model.layers.28.mlp.up_proj.weight is on meta\n",
      "model.layers.28.mlp.down_proj.weight is on meta\n",
      "model.layers.28.input_layernorm.weight is on meta\n",
      "model.layers.28.post_attention_layernorm.weight is on meta\n",
      "model.layers.29.self_attn.q_proj.weight is on meta\n",
      "model.layers.29.self_attn.k_proj.weight is on meta\n",
      "model.layers.29.self_attn.v_proj.weight is on meta\n",
      "model.layers.29.self_attn.o_proj.weight is on meta\n",
      "model.layers.29.mlp.gate_proj.weight is on meta\n",
      "model.layers.29.mlp.up_proj.weight is on meta\n",
      "model.layers.29.mlp.down_proj.weight is on meta\n",
      "model.layers.29.input_layernorm.weight is on meta\n",
      "model.layers.29.post_attention_layernorm.weight is on meta\n",
      "model.layers.30.self_attn.q_proj.weight is on meta\n",
      "model.layers.30.self_attn.k_proj.weight is on meta\n",
      "model.layers.30.self_attn.v_proj.weight is on meta\n",
      "model.layers.30.self_attn.o_proj.weight is on meta\n",
      "model.layers.30.mlp.gate_proj.weight is on meta\n",
      "model.layers.30.mlp.up_proj.weight is on meta\n",
      "model.layers.30.mlp.down_proj.weight is on meta\n",
      "model.layers.30.input_layernorm.weight is on meta\n",
      "model.layers.30.post_attention_layernorm.weight is on meta\n",
      "model.layers.31.self_attn.q_proj.weight is on meta\n",
      "model.layers.31.self_attn.k_proj.weight is on meta\n",
      "model.layers.31.self_attn.v_proj.weight is on meta\n",
      "model.layers.31.self_attn.o_proj.weight is on meta\n",
      "model.layers.31.mlp.gate_proj.weight is on meta\n",
      "model.layers.31.mlp.up_proj.weight is on meta\n",
      "model.layers.31.mlp.down_proj.weight is on meta\n",
      "model.layers.31.input_layernorm.weight is on meta\n",
      "model.layers.31.post_attention_layernorm.weight is on meta\n",
      "model.layers.32.self_attn.q_proj.weight is on meta\n",
      "model.layers.32.self_attn.k_proj.weight is on meta\n",
      "model.layers.32.self_attn.v_proj.weight is on meta\n",
      "model.layers.32.self_attn.o_proj.weight is on meta\n",
      "model.layers.32.mlp.gate_proj.weight is on meta\n",
      "model.layers.32.mlp.up_proj.weight is on meta\n",
      "model.layers.32.mlp.down_proj.weight is on meta\n",
      "model.layers.32.input_layernorm.weight is on meta\n",
      "model.layers.32.post_attention_layernorm.weight is on meta\n",
      "model.layers.33.self_attn.q_proj.weight is on meta\n",
      "model.layers.33.self_attn.k_proj.weight is on meta\n",
      "model.layers.33.self_attn.v_proj.weight is on meta\n",
      "model.layers.33.self_attn.o_proj.weight is on meta\n",
      "model.layers.33.mlp.gate_proj.weight is on meta\n",
      "model.layers.33.mlp.up_proj.weight is on meta\n",
      "model.layers.33.mlp.down_proj.weight is on meta\n",
      "model.layers.33.input_layernorm.weight is on meta\n",
      "model.layers.33.post_attention_layernorm.weight is on meta\n",
      "model.layers.34.self_attn.q_proj.weight is on meta\n",
      "model.layers.34.self_attn.k_proj.weight is on meta\n",
      "model.layers.34.self_attn.v_proj.weight is on meta\n",
      "model.layers.34.self_attn.o_proj.weight is on meta\n",
      "model.layers.34.mlp.gate_proj.weight is on meta\n",
      "model.layers.34.mlp.up_proj.weight is on meta\n",
      "model.layers.34.mlp.down_proj.weight is on meta\n",
      "model.layers.34.input_layernorm.weight is on meta\n",
      "model.layers.34.post_attention_layernorm.weight is on meta\n",
      "model.layers.35.self_attn.q_proj.weight is on meta\n",
      "model.layers.35.self_attn.k_proj.weight is on meta\n",
      "model.layers.35.self_attn.v_proj.weight is on meta\n",
      "model.layers.35.self_attn.o_proj.weight is on meta\n",
      "model.layers.35.mlp.gate_proj.weight is on meta\n",
      "model.layers.35.mlp.up_proj.weight is on meta\n",
      "model.layers.35.mlp.down_proj.weight is on meta\n",
      "model.layers.35.input_layernorm.weight is on meta\n",
      "model.layers.35.post_attention_layernorm.weight is on meta\n",
      "model.layers.36.self_attn.q_proj.weight is on meta\n",
      "model.layers.36.self_attn.k_proj.weight is on meta\n",
      "model.layers.36.self_attn.v_proj.weight is on meta\n",
      "model.layers.36.self_attn.o_proj.weight is on meta\n",
      "model.layers.36.mlp.gate_proj.weight is on meta\n",
      "model.layers.36.mlp.up_proj.weight is on meta\n",
      "model.layers.36.mlp.down_proj.weight is on meta\n",
      "model.layers.36.input_layernorm.weight is on meta\n",
      "model.layers.36.post_attention_layernorm.weight is on meta\n",
      "model.layers.37.self_attn.q_proj.weight is on meta\n",
      "model.layers.37.self_attn.k_proj.weight is on meta\n",
      "model.layers.37.self_attn.v_proj.weight is on meta\n",
      "model.layers.37.self_attn.o_proj.weight is on meta\n",
      "model.layers.37.mlp.gate_proj.weight is on meta\n",
      "model.layers.37.mlp.up_proj.weight is on meta\n",
      "model.layers.37.mlp.down_proj.weight is on meta\n",
      "model.layers.37.input_layernorm.weight is on meta\n",
      "model.layers.37.post_attention_layernorm.weight is on meta\n",
      "model.layers.38.self_attn.q_proj.weight is on meta\n",
      "model.layers.38.self_attn.k_proj.weight is on meta\n",
      "model.layers.38.self_attn.v_proj.weight is on meta\n",
      "model.layers.38.self_attn.o_proj.weight is on meta\n",
      "model.layers.38.mlp.gate_proj.weight is on meta\n",
      "model.layers.38.mlp.up_proj.weight is on meta\n",
      "model.layers.38.mlp.down_proj.weight is on meta\n",
      "model.layers.38.input_layernorm.weight is on meta\n",
      "model.layers.38.post_attention_layernorm.weight is on meta\n",
      "model.layers.39.self_attn.q_proj.weight is on meta\n",
      "model.layers.39.self_attn.k_proj.weight is on meta\n",
      "model.layers.39.self_attn.v_proj.weight is on meta\n",
      "model.layers.39.self_attn.o_proj.weight is on meta\n",
      "model.layers.39.mlp.gate_proj.weight is on meta\n",
      "model.layers.39.mlp.up_proj.weight is on meta\n",
      "model.layers.39.mlp.down_proj.weight is on meta\n",
      "model.layers.39.input_layernorm.weight is on meta\n",
      "model.layers.39.post_attention_layernorm.weight is on meta\n",
      "model.layers.40.self_attn.q_proj.weight is on meta\n",
      "model.layers.40.self_attn.k_proj.weight is on meta\n",
      "model.layers.40.self_attn.v_proj.weight is on meta\n",
      "model.layers.40.self_attn.o_proj.weight is on meta\n",
      "model.layers.40.mlp.gate_proj.weight is on meta\n",
      "model.layers.40.mlp.up_proj.weight is on meta\n",
      "model.layers.40.mlp.down_proj.weight is on meta\n",
      "model.layers.40.input_layernorm.weight is on meta\n",
      "model.layers.40.post_attention_layernorm.weight is on meta\n",
      "model.layers.41.self_attn.q_proj.weight is on meta\n",
      "model.layers.41.self_attn.k_proj.weight is on meta\n",
      "model.layers.41.self_attn.v_proj.weight is on meta\n",
      "model.layers.41.self_attn.o_proj.weight is on meta\n",
      "model.layers.41.mlp.gate_proj.weight is on meta\n",
      "model.layers.41.mlp.up_proj.weight is on meta\n",
      "model.layers.41.mlp.down_proj.weight is on meta\n",
      "model.layers.41.input_layernorm.weight is on meta\n",
      "model.layers.41.post_attention_layernorm.weight is on meta\n",
      "model.layers.42.self_attn.q_proj.weight is on meta\n",
      "model.layers.42.self_attn.k_proj.weight is on meta\n",
      "model.layers.42.self_attn.v_proj.weight is on meta\n",
      "model.layers.42.self_attn.o_proj.weight is on meta\n",
      "model.layers.42.mlp.gate_proj.weight is on meta\n",
      "model.layers.42.mlp.up_proj.weight is on meta\n",
      "model.layers.42.mlp.down_proj.weight is on meta\n",
      "model.layers.42.input_layernorm.weight is on meta\n",
      "model.layers.42.post_attention_layernorm.weight is on meta\n",
      "model.layers.43.self_attn.q_proj.weight is on meta\n",
      "model.layers.43.self_attn.k_proj.weight is on meta\n",
      "model.layers.43.self_attn.v_proj.weight is on meta\n",
      "model.layers.43.self_attn.o_proj.weight is on meta\n",
      "model.layers.43.mlp.gate_proj.weight is on meta\n",
      "model.layers.43.mlp.up_proj.weight is on meta\n",
      "model.layers.43.mlp.down_proj.weight is on meta\n",
      "model.layers.43.input_layernorm.weight is on meta\n",
      "model.layers.43.post_attention_layernorm.weight is on meta\n",
      "model.layers.44.self_attn.q_proj.weight is on meta\n",
      "model.layers.44.self_attn.k_proj.weight is on meta\n",
      "model.layers.44.self_attn.v_proj.weight is on meta\n",
      "model.layers.44.self_attn.o_proj.weight is on meta\n",
      "model.layers.44.mlp.gate_proj.weight is on meta\n",
      "model.layers.44.mlp.up_proj.weight is on meta\n",
      "model.layers.44.mlp.down_proj.weight is on meta\n",
      "model.layers.44.input_layernorm.weight is on meta\n",
      "model.layers.44.post_attention_layernorm.weight is on meta\n",
      "model.layers.45.self_attn.q_proj.weight is on meta\n",
      "model.layers.45.self_attn.k_proj.weight is on meta\n",
      "model.layers.45.self_attn.v_proj.weight is on meta\n",
      "model.layers.45.self_attn.o_proj.weight is on meta\n",
      "model.layers.45.mlp.gate_proj.weight is on meta\n",
      "model.layers.45.mlp.up_proj.weight is on meta\n",
      "model.layers.45.mlp.down_proj.weight is on meta\n",
      "model.layers.45.input_layernorm.weight is on meta\n",
      "model.layers.45.post_attention_layernorm.weight is on meta\n",
      "model.layers.46.self_attn.q_proj.weight is on meta\n",
      "model.layers.46.self_attn.k_proj.weight is on meta\n",
      "model.layers.46.self_attn.v_proj.weight is on meta\n",
      "model.layers.46.self_attn.o_proj.weight is on meta\n",
      "model.layers.46.mlp.gate_proj.weight is on meta\n",
      "model.layers.46.mlp.up_proj.weight is on meta\n",
      "model.layers.46.mlp.down_proj.weight is on meta\n",
      "model.layers.46.input_layernorm.weight is on meta\n",
      "model.layers.46.post_attention_layernorm.weight is on meta\n",
      "model.layers.47.self_attn.q_proj.weight is on meta\n",
      "model.layers.47.self_attn.k_proj.weight is on meta\n",
      "model.layers.47.self_attn.v_proj.weight is on meta\n",
      "model.layers.47.self_attn.o_proj.weight is on meta\n",
      "model.layers.47.mlp.gate_proj.weight is on meta\n",
      "model.layers.47.mlp.up_proj.weight is on meta\n",
      "model.layers.47.mlp.down_proj.weight is on meta\n",
      "model.layers.47.input_layernorm.weight is on meta\n",
      "model.layers.47.post_attention_layernorm.weight is on meta\n",
      "model.layers.48.self_attn.q_proj.weight is on meta\n",
      "model.layers.48.self_attn.k_proj.weight is on meta\n",
      "model.layers.48.self_attn.v_proj.weight is on meta\n",
      "model.layers.48.self_attn.o_proj.weight is on meta\n",
      "model.layers.48.mlp.gate_proj.weight is on meta\n",
      "model.layers.48.mlp.up_proj.weight is on meta\n",
      "model.layers.48.mlp.down_proj.weight is on meta\n",
      "model.layers.48.input_layernorm.weight is on meta\n",
      "model.layers.48.post_attention_layernorm.weight is on meta\n",
      "model.layers.49.self_attn.q_proj.weight is on meta\n",
      "model.layers.49.self_attn.k_proj.weight is on meta\n",
      "model.layers.49.self_attn.v_proj.weight is on meta\n",
      "model.layers.49.self_attn.o_proj.weight is on meta\n",
      "model.layers.49.mlp.gate_proj.weight is on meta\n",
      "model.layers.49.mlp.up_proj.weight is on meta\n",
      "model.layers.49.mlp.down_proj.weight is on meta\n",
      "model.layers.49.input_layernorm.weight is on meta\n",
      "model.layers.49.post_attention_layernorm.weight is on meta\n",
      "model.layers.50.self_attn.q_proj.weight is on meta\n",
      "model.layers.50.self_attn.k_proj.weight is on meta\n",
      "model.layers.50.self_attn.v_proj.weight is on meta\n",
      "model.layers.50.self_attn.o_proj.weight is on meta\n",
      "model.layers.50.mlp.gate_proj.weight is on meta\n",
      "model.layers.50.mlp.up_proj.weight is on meta\n",
      "model.layers.50.mlp.down_proj.weight is on meta\n",
      "model.layers.50.input_layernorm.weight is on meta\n",
      "model.layers.50.post_attention_layernorm.weight is on meta\n",
      "model.layers.51.self_attn.q_proj.weight is on meta\n",
      "model.layers.51.self_attn.k_proj.weight is on meta\n",
      "model.layers.51.self_attn.v_proj.weight is on meta\n",
      "model.layers.51.self_attn.o_proj.weight is on meta\n",
      "model.layers.51.mlp.gate_proj.weight is on meta\n",
      "model.layers.51.mlp.up_proj.weight is on meta\n",
      "model.layers.51.mlp.down_proj.weight is on meta\n",
      "model.layers.51.input_layernorm.weight is on meta\n",
      "model.layers.51.post_attention_layernorm.weight is on meta\n",
      "model.layers.52.self_attn.q_proj.weight is on meta\n",
      "model.layers.52.self_attn.k_proj.weight is on meta\n",
      "model.layers.52.self_attn.v_proj.weight is on meta\n",
      "model.layers.52.self_attn.o_proj.weight is on meta\n",
      "model.layers.52.mlp.gate_proj.weight is on meta\n",
      "model.layers.52.mlp.up_proj.weight is on meta\n",
      "model.layers.52.mlp.down_proj.weight is on meta\n",
      "model.layers.52.input_layernorm.weight is on meta\n",
      "model.layers.52.post_attention_layernorm.weight is on meta\n",
      "model.layers.53.self_attn.q_proj.weight is on meta\n",
      "model.layers.53.self_attn.k_proj.weight is on meta\n",
      "model.layers.53.self_attn.v_proj.weight is on meta\n",
      "model.layers.53.self_attn.o_proj.weight is on meta\n",
      "model.layers.53.mlp.gate_proj.weight is on meta\n",
      "model.layers.53.mlp.up_proj.weight is on meta\n",
      "model.layers.53.mlp.down_proj.weight is on meta\n",
      "model.layers.53.input_layernorm.weight is on meta\n",
      "model.layers.53.post_attention_layernorm.weight is on meta\n",
      "model.layers.54.self_attn.q_proj.weight is on meta\n",
      "model.layers.54.self_attn.k_proj.weight is on meta\n",
      "model.layers.54.self_attn.v_proj.weight is on meta\n",
      "model.layers.54.self_attn.o_proj.weight is on meta\n",
      "model.layers.54.mlp.gate_proj.weight is on meta\n",
      "model.layers.54.mlp.up_proj.weight is on meta\n",
      "model.layers.54.mlp.down_proj.weight is on meta\n",
      "model.layers.54.input_layernorm.weight is on meta\n",
      "model.layers.54.post_attention_layernorm.weight is on meta\n",
      "model.layers.55.self_attn.q_proj.weight is on meta\n",
      "model.layers.55.self_attn.k_proj.weight is on meta\n",
      "model.layers.55.self_attn.v_proj.weight is on meta\n",
      "model.layers.55.self_attn.o_proj.weight is on meta\n",
      "model.layers.55.mlp.gate_proj.weight is on meta\n",
      "model.layers.55.mlp.up_proj.weight is on meta\n",
      "model.layers.55.mlp.down_proj.weight is on meta\n",
      "model.layers.55.input_layernorm.weight is on meta\n",
      "model.layers.55.post_attention_layernorm.weight is on meta\n",
      "model.layers.56.self_attn.q_proj.weight is on meta\n",
      "model.layers.56.self_attn.k_proj.weight is on meta\n",
      "model.layers.56.self_attn.v_proj.weight is on meta\n",
      "model.layers.56.self_attn.o_proj.weight is on meta\n",
      "model.layers.56.mlp.gate_proj.weight is on meta\n",
      "model.layers.56.mlp.up_proj.weight is on meta\n",
      "model.layers.56.mlp.down_proj.weight is on meta\n",
      "model.layers.56.input_layernorm.weight is on meta\n",
      "model.layers.56.post_attention_layernorm.weight is on meta\n",
      "model.layers.57.self_attn.q_proj.weight is on meta\n",
      "model.layers.57.self_attn.k_proj.weight is on meta\n",
      "model.layers.57.self_attn.v_proj.weight is on meta\n",
      "model.layers.57.self_attn.o_proj.weight is on meta\n",
      "model.layers.57.mlp.gate_proj.weight is on meta\n",
      "model.layers.57.mlp.up_proj.weight is on meta\n",
      "model.layers.57.mlp.down_proj.weight is on meta\n",
      "model.layers.57.input_layernorm.weight is on meta\n",
      "model.layers.57.post_attention_layernorm.weight is on meta\n",
      "model.layers.58.self_attn.q_proj.weight is on meta\n",
      "model.layers.58.self_attn.k_proj.weight is on meta\n",
      "model.layers.58.self_attn.v_proj.weight is on meta\n",
      "model.layers.58.self_attn.o_proj.weight is on meta\n",
      "model.layers.58.mlp.gate_proj.weight is on meta\n",
      "model.layers.58.mlp.up_proj.weight is on meta\n",
      "model.layers.58.mlp.down_proj.weight is on meta\n",
      "model.layers.58.input_layernorm.weight is on meta\n",
      "model.layers.58.post_attention_layernorm.weight is on meta\n",
      "model.layers.59.self_attn.q_proj.weight is on meta\n",
      "model.layers.59.self_attn.k_proj.weight is on meta\n",
      "model.layers.59.self_attn.v_proj.weight is on meta\n",
      "model.layers.59.self_attn.o_proj.weight is on meta\n",
      "model.layers.59.mlp.gate_proj.weight is on meta\n",
      "model.layers.59.mlp.up_proj.weight is on meta\n",
      "model.layers.59.mlp.down_proj.weight is on meta\n",
      "model.layers.59.input_layernorm.weight is on meta\n",
      "model.layers.59.post_attention_layernorm.weight is on meta\n",
      "model.layers.60.self_attn.q_proj.weight is on meta\n",
      "model.layers.60.self_attn.k_proj.weight is on meta\n",
      "model.layers.60.self_attn.v_proj.weight is on meta\n",
      "model.layers.60.self_attn.o_proj.weight is on meta\n",
      "model.layers.60.mlp.gate_proj.weight is on meta\n",
      "model.layers.60.mlp.up_proj.weight is on meta\n",
      "model.layers.60.mlp.down_proj.weight is on meta\n",
      "model.layers.60.input_layernorm.weight is on meta\n",
      "model.layers.60.post_attention_layernorm.weight is on meta\n",
      "model.layers.61.self_attn.q_proj.weight is on meta\n",
      "model.layers.61.self_attn.k_proj.weight is on meta\n",
      "model.layers.61.self_attn.v_proj.weight is on meta\n",
      "model.layers.61.self_attn.o_proj.weight is on meta\n",
      "model.layers.61.mlp.gate_proj.weight is on meta\n",
      "model.layers.61.mlp.up_proj.weight is on meta\n",
      "model.layers.61.mlp.down_proj.weight is on meta\n",
      "model.layers.61.input_layernorm.weight is on meta\n",
      "model.layers.61.post_attention_layernorm.weight is on meta\n",
      "model.layers.62.self_attn.q_proj.weight is on meta\n",
      "model.layers.62.self_attn.k_proj.weight is on meta\n",
      "model.layers.62.self_attn.v_proj.weight is on meta\n",
      "model.layers.62.self_attn.o_proj.weight is on meta\n",
      "model.layers.62.mlp.gate_proj.weight is on meta\n",
      "model.layers.62.mlp.up_proj.weight is on meta\n",
      "model.layers.62.mlp.down_proj.weight is on meta\n",
      "model.layers.62.input_layernorm.weight is on meta\n",
      "model.layers.62.post_attention_layernorm.weight is on meta\n",
      "model.layers.63.self_attn.q_proj.weight is on meta\n",
      "model.layers.63.self_attn.k_proj.weight is on meta\n",
      "model.layers.63.self_attn.v_proj.weight is on meta\n",
      "model.layers.63.self_attn.o_proj.weight is on meta\n",
      "model.layers.63.mlp.gate_proj.weight is on meta\n",
      "model.layers.63.mlp.up_proj.weight is on meta\n",
      "model.layers.63.mlp.down_proj.weight is on meta\n",
      "model.layers.63.input_layernorm.weight is on meta\n",
      "model.layers.63.post_attention_layernorm.weight is on meta\n",
      "model.layers.64.self_attn.q_proj.weight is on meta\n",
      "model.layers.64.self_attn.k_proj.weight is on meta\n",
      "model.layers.64.self_attn.v_proj.weight is on meta\n",
      "model.layers.64.self_attn.o_proj.weight is on meta\n",
      "model.layers.64.mlp.gate_proj.weight is on meta\n",
      "model.layers.64.mlp.up_proj.weight is on meta\n",
      "model.layers.64.mlp.down_proj.weight is on meta\n",
      "model.layers.64.input_layernorm.weight is on meta\n",
      "model.layers.64.post_attention_layernorm.weight is on meta\n",
      "model.layers.65.self_attn.q_proj.weight is on meta\n",
      "model.layers.65.self_attn.k_proj.weight is on meta\n",
      "model.layers.65.self_attn.v_proj.weight is on meta\n",
      "model.layers.65.self_attn.o_proj.weight is on meta\n",
      "model.layers.65.mlp.gate_proj.weight is on meta\n",
      "model.layers.65.mlp.up_proj.weight is on meta\n",
      "model.layers.65.mlp.down_proj.weight is on meta\n",
      "model.layers.65.input_layernorm.weight is on meta\n",
      "model.layers.65.post_attention_layernorm.weight is on meta\n",
      "model.layers.66.self_attn.q_proj.weight is on meta\n",
      "model.layers.66.self_attn.k_proj.weight is on meta\n",
      "model.layers.66.self_attn.v_proj.weight is on meta\n",
      "model.layers.66.self_attn.o_proj.weight is on meta\n",
      "model.layers.66.mlp.gate_proj.weight is on meta\n",
      "model.layers.66.mlp.up_proj.weight is on meta\n",
      "model.layers.66.mlp.down_proj.weight is on meta\n",
      "model.layers.66.input_layernorm.weight is on meta\n",
      "model.layers.66.post_attention_layernorm.weight is on meta\n",
      "model.layers.67.self_attn.q_proj.weight is on meta\n",
      "model.layers.67.self_attn.k_proj.weight is on meta\n",
      "model.layers.67.self_attn.v_proj.weight is on meta\n",
      "model.layers.67.self_attn.o_proj.weight is on meta\n",
      "model.layers.67.mlp.gate_proj.weight is on meta\n",
      "model.layers.67.mlp.up_proj.weight is on meta\n",
      "model.layers.67.mlp.down_proj.weight is on meta\n",
      "model.layers.67.input_layernorm.weight is on meta\n",
      "model.layers.67.post_attention_layernorm.weight is on meta\n",
      "model.layers.68.self_attn.q_proj.weight is on meta\n",
      "model.layers.68.self_attn.k_proj.weight is on meta\n",
      "model.layers.68.self_attn.v_proj.weight is on meta\n",
      "model.layers.68.self_attn.o_proj.weight is on meta\n",
      "model.layers.68.mlp.gate_proj.weight is on meta\n",
      "model.layers.68.mlp.up_proj.weight is on meta\n",
      "model.layers.68.mlp.down_proj.weight is on meta\n",
      "model.layers.68.input_layernorm.weight is on meta\n",
      "model.layers.68.post_attention_layernorm.weight is on meta\n",
      "model.layers.69.self_attn.q_proj.weight is on meta\n",
      "model.layers.69.self_attn.k_proj.weight is on meta\n",
      "model.layers.69.self_attn.v_proj.weight is on meta\n",
      "model.layers.69.self_attn.o_proj.weight is on meta\n",
      "model.layers.69.mlp.gate_proj.weight is on meta\n",
      "model.layers.69.mlp.up_proj.weight is on meta\n",
      "model.layers.69.mlp.down_proj.weight is on meta\n",
      "model.layers.69.input_layernorm.weight is on meta\n",
      "model.layers.69.post_attention_layernorm.weight is on meta\n",
      "model.layers.70.self_attn.q_proj.weight is on meta\n",
      "model.layers.70.self_attn.k_proj.weight is on meta\n",
      "model.layers.70.self_attn.v_proj.weight is on meta\n",
      "model.layers.70.self_attn.o_proj.weight is on meta\n",
      "model.layers.70.mlp.gate_proj.weight is on meta\n",
      "model.layers.70.mlp.up_proj.weight is on meta\n",
      "model.layers.70.mlp.down_proj.weight is on meta\n",
      "model.layers.70.input_layernorm.weight is on meta\n",
      "model.layers.70.post_attention_layernorm.weight is on meta\n",
      "model.layers.71.self_attn.q_proj.weight is on meta\n",
      "model.layers.71.self_attn.k_proj.weight is on meta\n",
      "model.layers.71.self_attn.v_proj.weight is on meta\n",
      "model.layers.71.self_attn.o_proj.weight is on meta\n",
      "model.layers.71.mlp.gate_proj.weight is on meta\n",
      "model.layers.71.mlp.up_proj.weight is on meta\n",
      "model.layers.71.mlp.down_proj.weight is on meta\n",
      "model.layers.71.input_layernorm.weight is on meta\n",
      "model.layers.71.post_attention_layernorm.weight is on meta\n",
      "model.layers.72.self_attn.q_proj.weight is on meta\n",
      "model.layers.72.self_attn.k_proj.weight is on meta\n",
      "model.layers.72.self_attn.v_proj.weight is on meta\n",
      "model.layers.72.self_attn.o_proj.weight is on meta\n",
      "model.layers.72.mlp.gate_proj.weight is on meta\n",
      "model.layers.72.mlp.up_proj.weight is on meta\n",
      "model.layers.72.mlp.down_proj.weight is on meta\n",
      "model.layers.72.input_layernorm.weight is on meta\n",
      "model.layers.72.post_attention_layernorm.weight is on meta\n",
      "model.layers.73.self_attn.q_proj.weight is on meta\n",
      "model.layers.73.self_attn.k_proj.weight is on meta\n",
      "model.layers.73.self_attn.v_proj.weight is on meta\n",
      "model.layers.73.self_attn.o_proj.weight is on meta\n",
      "model.layers.73.mlp.gate_proj.weight is on meta\n",
      "model.layers.73.mlp.up_proj.weight is on meta\n",
      "model.layers.73.mlp.down_proj.weight is on meta\n",
      "model.layers.73.input_layernorm.weight is on meta\n",
      "model.layers.73.post_attention_layernorm.weight is on meta\n",
      "model.layers.74.self_attn.q_proj.weight is on meta\n",
      "model.layers.74.self_attn.k_proj.weight is on meta\n",
      "model.layers.74.self_attn.v_proj.weight is on meta\n",
      "model.layers.74.self_attn.o_proj.weight is on meta\n",
      "model.layers.74.mlp.gate_proj.weight is on meta\n",
      "model.layers.74.mlp.up_proj.weight is on meta\n",
      "model.layers.74.mlp.down_proj.weight is on meta\n",
      "model.layers.74.input_layernorm.weight is on meta\n",
      "model.layers.74.post_attention_layernorm.weight is on meta\n",
      "model.layers.75.self_attn.q_proj.weight is on meta\n",
      "model.layers.75.self_attn.k_proj.weight is on meta\n",
      "model.layers.75.self_attn.v_proj.weight is on meta\n",
      "model.layers.75.self_attn.o_proj.weight is on meta\n",
      "model.layers.75.mlp.gate_proj.weight is on meta\n",
      "model.layers.75.mlp.up_proj.weight is on meta\n",
      "model.layers.75.mlp.down_proj.weight is on meta\n",
      "model.layers.75.input_layernorm.weight is on meta\n",
      "model.layers.75.post_attention_layernorm.weight is on meta\n",
      "model.layers.76.self_attn.q_proj.weight is on meta\n",
      "model.layers.76.self_attn.k_proj.weight is on meta\n",
      "model.layers.76.self_attn.v_proj.weight is on meta\n",
      "model.layers.76.self_attn.o_proj.weight is on meta\n",
      "model.layers.76.mlp.gate_proj.weight is on meta\n",
      "model.layers.76.mlp.up_proj.weight is on meta\n",
      "model.layers.76.mlp.down_proj.weight is on meta\n",
      "model.layers.76.input_layernorm.weight is on meta\n",
      "model.layers.76.post_attention_layernorm.weight is on meta\n",
      "model.layers.77.self_attn.q_proj.weight is on meta\n",
      "model.layers.77.self_attn.k_proj.weight is on meta\n",
      "model.layers.77.self_attn.v_proj.weight is on meta\n",
      "model.layers.77.self_attn.o_proj.weight is on meta\n",
      "model.layers.77.mlp.gate_proj.weight is on meta\n",
      "model.layers.77.mlp.up_proj.weight is on meta\n",
      "model.layers.77.mlp.down_proj.weight is on meta\n",
      "model.layers.77.input_layernorm.weight is on meta\n",
      "model.layers.77.post_attention_layernorm.weight is on meta\n",
      "model.layers.78.self_attn.q_proj.weight is on meta\n",
      "model.layers.78.self_attn.k_proj.weight is on meta\n",
      "model.layers.78.self_attn.v_proj.weight is on meta\n",
      "model.layers.78.self_attn.o_proj.weight is on meta\n",
      "model.layers.78.mlp.gate_proj.weight is on meta\n",
      "model.layers.78.mlp.up_proj.weight is on meta\n",
      "model.layers.78.mlp.down_proj.weight is on meta\n",
      "model.layers.78.input_layernorm.weight is on meta\n",
      "model.layers.78.post_attention_layernorm.weight is on meta\n",
      "model.layers.79.self_attn.q_proj.weight is on meta\n",
      "model.layers.79.self_attn.k_proj.weight is on meta\n",
      "model.layers.79.self_attn.v_proj.weight is on meta\n",
      "model.layers.79.self_attn.o_proj.weight is on meta\n",
      "model.layers.79.mlp.gate_proj.weight is on meta\n",
      "model.layers.79.mlp.up_proj.weight is on meta\n",
      "model.layers.79.mlp.down_proj.weight is on meta\n",
      "model.layers.79.input_layernorm.weight is on meta\n",
      "model.layers.79.post_attention_layernorm.weight is on meta\n",
      "model.norm.weight is on meta\n",
      "lm_head.weight is on meta\n"
     ]
    }
   ],
   "source": [
    "# For model_0\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name} is on {param.device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(65140, 5)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "answer_df = pd.read_csv(\"../../llama_data/answer_df_part1.csv\")\n",
    "query_df = pd.read_csv(\"../../llama_data/query_df3.csv\")\n",
    "print(answer_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "\n",
    "\n",
    "system_message = \"\"\"\n",
    "You are an AI assistant designed to answer questions.\n",
    "Please restrict your answer to the exact question and use the exact answer format asked.\n",
    "Answer should not have implied information. If the answer is yes, always provide related phrases. \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def colorize_text(text):\n",
    "    for word, color in zip([\"Reasoning\", \"Question\", \"Answer\", \"Total time\"], [\"blue\", \"red\", \"green\", \"magenta\"]):\n",
    "        text = text.replace(f\"{word}:\", f\"\\n\\n**<font color='{color}'>{word}:</font>**\")\n",
    "    return text\n",
    "\n",
    "def format_prompt(text):\n",
    "    question_content = \"Does the paragraph mention any of the following topics:\\n\"\n",
    "    for i in range(len(query_df)):\n",
    "        question_content += f\"  ({i+1}) {query_df.topic[i]}: {query_df.description[i]}.\\n\"\n",
    "    answer_content = \"Return answer in format:\\n\"\n",
    "    for i in range(len(query_df)): \n",
    "        answer_content += f\"  ({i+1}) {query_df.topic[i]}: [yes/no], related phrases if any: \\n\"\n",
    "    paragragh_content = f\"Paragraph: '{text}' \\n\"\n",
    "    user_message = question_content + answer_content + paragragh_content\n",
    "    #print(user_message)\n",
    "    \n",
    "    return user_message\n",
    "\n",
    "\n",
    "\n",
    "def query_model_batch(\n",
    "        system_message,\n",
    "        user_messages,\n",
    "        temperature=0,\n",
    "        max_length=1024\n",
    "    ):\n",
    "    start_time = time()\n",
    "    # Add \"Question: ... Answer:\" to each user message for clarity\n",
    "    batched_messages = [\n",
    "        \"Question: \" + message + \" Answer:\" for message in user_messages\n",
    "    ]\n",
    "    \n",
    "    # Construct prompts for each message in batch\n",
    "    all_prompts = [\n",
    "        pipe.tokenizer.apply_chat_template(\n",
    "            [\n",
    "                {\"role\": \"system\", \"content\": system_message},\n",
    "                {\"role\": \"user\", \"content\": user_message}\n",
    "            ],\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        ) for user_message in batched_messages\n",
    "    ]\n",
    "    \n",
    "    # Define the end-of-sequence terminators\n",
    "    terminators = [\n",
    "        pipe.tokenizer.eos_token_id,\n",
    "        pipe.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "    ]\n",
    "    \n",
    "    # Run the batch inference\n",
    "    sequences = pipe(\n",
    "        all_prompts,\n",
    "        do_sample=True,\n",
    "        top_p=0.5,\n",
    "        temperature=temperature,\n",
    "        num_return_sequences=1,\n",
    "        eos_token_id=terminators,\n",
    "        max_new_tokens=max_length,\n",
    "        return_full_text=False,\n",
    "        pad_token_id=terminators[0]\n",
    "    )\n",
    "    \n",
    "    # Extract generated text for each sequence\n",
    "    answers = []\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        answer = sequence[0]['generated_text']\n",
    "        total_time = f\"Total time: {round(time() - start_time, 2)} sec.\"\n",
    "        # Format the response with timing information\n",
    "        answers.append(batched_messages[i] + \" \" + answer + \" \" + total_time)\n",
    "\n",
    "    return answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.96 GiB. GPU 0 has a total capacity of 10.75 GiB of which 1.27 GiB is free. Process 3684134 has 228.00 MiB memory in use. Process 349965 has 246.00 MiB memory in use. Including non-PyTorch memory, this process has 8.99 GiB memory in use. Of the allocated memory 8.78 GiB is allocated by PyTorch, and 30.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mi have always wondered how people with ana make it when theyre eating very few calorie s. do their organs not shut down. i suffered with an eating disorder for a few year s and i am recovered now but i never struggled with severe anorexia. does the body only begin to cannibalize the organs after all of their fat storage is gone. what is the process.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m prompts \u001b[38;5;241m=\u001b[39m [format_prompt(text)]\n\u001b[0;32m----> 7\u001b[0m responses \u001b[38;5;241m=\u001b[39m \u001b[43mquery_model_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43msystem_message\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msystem_message\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_messages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Display and process responses in a loop\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, response \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(responses):\n",
      "Cell \u001b[0;32mIn[8], line 63\u001b[0m, in \u001b[0;36mquery_model_batch\u001b[0;34m(system_message, user_messages, temperature, max_length)\u001b[0m\n\u001b[1;32m     57\u001b[0m terminators \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     58\u001b[0m     pipe\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39meos_token_id,\n\u001b[1;32m     59\u001b[0m     pipe\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mconvert_tokens_to_ids(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<|eot_id|>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     60\u001b[0m ]\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# Run the batch inference\u001b[39;00m\n\u001b[0;32m---> 63\u001b[0m sequences \u001b[38;5;241m=\u001b[39m \u001b[43mpipe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m    \u001b[49m\u001b[43mall_prompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_return_sequences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mterminators\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_full_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mterminators\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m# Extract generated text for each sequence\u001b[39;00m\n\u001b[1;32m     76\u001b[0m answers \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/miniconda/ENTER/envs/llama/lib/python3.10/site-packages/transformers/pipelines/text_generation.py:272\u001b[0m, in \u001b[0;36mTextGenerationPipeline.__call__\u001b[0;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    270\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(chats, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 272\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtext_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda/ENTER/envs/llama/lib/python3.10/site-packages/transformers/pipelines/base.py:1249\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1245\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m can_use_iterator:\n\u001b[1;32m   1246\u001b[0m     final_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[1;32m   1247\u001b[0m         inputs, num_workers, batch_size, preprocess_params, forward_params, postprocess_params\n\u001b[1;32m   1248\u001b[0m     )\n\u001b[0;32m-> 1249\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfinal_iterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1250\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n\u001b[1;32m   1251\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda/ENTER/envs/llama/lib/python3.10/site-packages/transformers/pipelines/pt_utils.py:124\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_item()\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer(item, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda/ENTER/envs/llama/lib/python3.10/site-packages/transformers/pipelines/pt_utils.py:125\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[1;32m    124\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterator)\n\u001b[0;32m--> 125\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;66;03m# Try to infer the size of the batch\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda/ENTER/envs/llama/lib/python3.10/site-packages/transformers/pipelines/base.py:1175\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1173\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1174\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m-> 1175\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1176\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m   1177\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda/ENTER/envs/llama/lib/python3.10/site-packages/transformers/pipelines/text_generation.py:370\u001b[0m, in \u001b[0;36mTextGenerationPipeline._forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    367\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration_config\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m generate_kwargs:\n\u001b[1;32m    368\u001b[0m     generate_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration_config\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeneration_config\n\u001b[0;32m--> 370\u001b[0m generated_sequence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    371\u001b[0m out_b \u001b[38;5;241m=\u001b[39m generated_sequence\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    372\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda/ENTER/envs/llama/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda/ENTER/envs/llama/lib/python3.10/site-packages/transformers/generation/utils.py:2047\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2039\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2040\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2041\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2042\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2043\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2044\u001b[0m     )\n\u001b[1;32m   2046\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2047\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2048\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2049\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2050\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2051\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2052\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2053\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2054\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2055\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2057\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2058\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   2059\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   2060\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   2061\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2066\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   2067\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda/ENTER/envs/llama/lib/python3.10/site-packages/transformers/generation/utils.py:3007\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3004\u001b[0m model_inputs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_hidden_states\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_hidden_states} \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[1;32m   3006\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 3007\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   3009\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   3010\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda/ENTER/envs/llama/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda/ENTER/envs/llama/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda/ENTER/envs/llama/lib/python3.10/site-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda/ENTER/envs/llama/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:1214\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, num_logits_to_keep)\u001b[0m\n\u001b[1;32m   1209\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning_once(\n\u001b[1;32m   1210\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1211\u001b[0m         )\n\u001b[1;32m   1212\u001b[0m     \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n\u001b[1;32m   1213\u001b[0m     \u001b[38;5;66;03m# TODO: remove the float() operation in v4.46\u001b[39;00m\n\u001b[0;32m-> 1214\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlm_head\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mnum_logits_to_keep\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m   1216\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1217\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1218\u001b[0m     \u001b[38;5;66;03m# Upcast to float if we need to compute the loss to avoid potential precision issues\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda/ENTER/envs/llama/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda/ENTER/envs/llama/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda/ENTER/envs/llama/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnew_forward\u001b[39m(module, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 165\u001b[0m     args, kwargs \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_hf_hook\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpre_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mno_grad:\n\u001b[1;32m    167\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m~/miniconda/ENTER/envs/llama/lib/python3.10/site-packages/accelerate/hooks.py:336\u001b[0m, in \u001b[0;36mAlignDevicesHook.pre_forward\u001b[0;34m(self, module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, _ \u001b[38;5;129;01min\u001b[39;00m named_module_tensors(\n\u001b[1;32m    330\u001b[0m     module,\n\u001b[1;32m    331\u001b[0m     include_buffers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moffload_buffers,\n\u001b[1;32m    332\u001b[0m     recurse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mplace_submodules,\n\u001b[1;32m    333\u001b[0m     remove_non_persistent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    334\u001b[0m ):\n\u001b[1;32m    335\u001b[0m     fp16_statistics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 336\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweights_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    337\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweight\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m name \u001b[38;5;129;01mand\u001b[39;00m name\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweight\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSCB\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights_map\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    338\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m value\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mint8:\n",
      "File \u001b[0;32m~/miniconda/ENTER/envs/llama/lib/python3.10/site-packages/accelerate/utils/offload.py:118\u001b[0m, in \u001b[0;36mPrefixedDataset.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[0;32m--> 118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprefix\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mkey\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m~/miniconda/ENTER/envs/llama/lib/python3.10/site-packages/accelerate/utils/offload.py:178\u001b[0m, in \u001b[0;36mOffloadedWeightsLoader.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    175\u001b[0m         tensor \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mget_tensor(weight_info\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweight_name\u001b[39m\u001b[38;5;124m\"\u001b[39m, key))\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m weight_info:\n\u001b[0;32m--> 178\u001b[0m     tensor \u001b[38;5;241m=\u001b[39m \u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_info\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdtype\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tensor\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m!=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(device):\n\u001b[1;32m    181\u001b[0m     tensor \u001b[38;5;241m=\u001b[39m tensor\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.96 GiB. GPU 0 has a total capacity of 10.75 GiB of which 1.27 GiB is free. Process 3684134 has 228.00 MiB memory in use. Process 349965 has 246.00 MiB memory in use. Including non-PyTorch memory, this process has 8.99 GiB memory in use. Of the allocated memory 8.78 GiB is allocated by PyTorch, and 30.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# test example\n",
    "\n",
    "text = \"i have always wondered how people with ana make it when theyre eating very few calorie s. do their organs not shut down. i suffered with an eating disorder for a few year s and i am recovered now but i never struggled with severe anorexia. does the body only begin to cannibalize the organs after all of their fat storage is gone. what is the process.\"\n",
    "\n",
    "\n",
    "prompts = [format_prompt(text)]\n",
    "responses = query_model_batch(\n",
    "        system_message=system_message,\n",
    "        user_messages=prompts,\n",
    "        temperature=0.5,\n",
    "        max_length=512\n",
    "    )\n",
    "\n",
    "\n",
    "# Display and process responses in a loop\n",
    "for i, response in enumerate(responses):\n",
    "    print(colorize_text(f\"{response}\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
