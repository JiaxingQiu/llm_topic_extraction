{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":104449,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":68809,"modelId":91102}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# %pip install -U transformers accelerate","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-17T13:59:10.323949Z","iopub.execute_input":"2024-10-17T13:59:10.324353Z","iopub.status.idle":"2024-10-17T13:59:10.329258Z","shell.execute_reply.started":"2024-10-17T13:59:10.324313Z","shell.execute_reply":"2024-10-17T13:59:10.328277Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"from time import time\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\nimport torch\nfrom IPython.display import display, Markdown\nimport pandas as pd","metadata":{"execution":{"iopub.status.busy":"2024-10-17T13:59:10.333150Z","iopub.execute_input":"2024-10-17T13:59:10.333442Z","iopub.status.idle":"2024-10-17T13:59:29.519779Z","shell.execute_reply.started":"2024-10-17T13:59:10.333410Z","shell.execute_reply":"2024-10-17T13:59:29.518777Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer,AutoModelForCausalLM,pipeline\nimport torch\n\nbase_model = \"/kaggle/input/llama-3.1/transformers/8b-instruct/2\"\n\ntokenizer = AutoTokenizer.from_pretrained(base_model)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n        base_model,\n        return_dict=True,\n        low_cpu_mem_usage=True,\n        torch_dtype=torch.float16,\n        device_map=\"auto\",\n        trust_remote_code=True,\n)\npipe = pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    torch_dtype=torch.float16,\n    device_map=\"auto\",\n)","metadata":{"execution":{"iopub.status.busy":"2024-10-17T13:59:29.521664Z","iopub.execute_input":"2024-10-17T13:59:29.522263Z","iopub.status.idle":"2024-10-17T14:00:46.888026Z","shell.execute_reply.started":"2024-10-17T13:59:29.522217Z","shell.execute_reply":"2024-10-17T14:00:46.887059Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9f00054628054fd5a78dcef287ebc98a"}},"metadata":{}}]},{"cell_type":"code","source":"def colorize_text(text):\n    for word, color in zip([\"Reasoning\", \"Question\", \"Answer\", \"Total time\"], [\"blue\", \"red\", \"green\", \"magenta\"]):\n        text = text.replace(f\"{word}:\", f\"\\n\\n**<font color='{color}'>{word}:</font>**\")\n    return text\n\n\ndef query_model(\n        system_message,\n        user_message,\n        temperature=0,\n        max_length=1024\n        ):\n    start_time = time()\n    user_message = \"Question: \" + user_message + \" Answer:\"\n    messages = [\n        {\"role\": \"system\", \"content\": system_message},\n        {\"role\": \"user\", \"content\": user_message},\n        ]\n    prompt = pipe.tokenizer.apply_chat_template(\n        messages, \n        tokenize=False, \n        add_generation_prompt=True\n        )\n    terminators = [\n        pipe.tokenizer.eos_token_id,\n        pipe.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n    ]\n    sequences = pipe(\n        prompt,\n        do_sample=True,\n        top_p=0.9,\n        temperature=temperature,\n        num_return_sequences=1,\n        eos_token_id=terminators,\n        max_new_tokens=max_length,\n        return_full_text=False,\n        pad_token_id=terminators[0]\n    )\n    #answer = f\"{sequences[0]['generated_text'][len(prompt):]}\\n\"\n    answer = sequences[0]['generated_text']\n    end_time = time()\n    ttime = f\"Total time: {round(end_time-start_time, 2)} sec.\"\n\n    return user_message + \" \" + answer  + \" \" +  ttime\n\n\nsystem_message = \"\"\"\nYou are an AI assistant designed to answer questions.\nPlease restrict your answer to the exact question and use the exact answer format asked.\n\"\"\"\n","metadata":{"execution":{"iopub.status.busy":"2024-10-17T14:00:46.889170Z","iopub.execute_input":"2024-10-17T14:00:46.889471Z","iopub.status.idle":"2024-10-17T14:00:46.898598Z","shell.execute_reply.started":"2024-10-17T14:00:46.889437Z","shell.execute_reply":"2024-10-17T14:00:46.897614Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# # test\n# t1 = time()\n# response = query_model(\n#     system_message,\n#     user_message=\"What is the surface temperature of the Moon?\",\n#     temperature=0.1,\n#     max_length=256)\n# display(Markdown(colorize_text(f\"{response}\")))","metadata":{"execution":{"iopub.status.busy":"2024-10-17T03:31:58.854114Z","iopub.execute_input":"2024-10-17T03:31:58.854493Z","iopub.status.idle":"2024-10-17T03:32:40.494684Z","shell.execute_reply.started":"2024-10-17T03:31:58.854455Z","shell.execute_reply":"2024-10-17T03:32:40.493761Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"\n\n**<font color='red'>Question:</font>** What is the surface temperature of the Moon? \n\n**<font color='green'>Answer:</font>** The surface temperature of the Moon varies greatly between day and night. The average temperature during the lunar day (when the Sun is shining on the surface) is around 107째C (225째F), while the average temperature during the lunar night (when the Sun is not shining on the surface) is around -173째C (-279째F). \n\n**<font color='magenta'>Total time:</font>** 41.63 sec."},"metadata":{}}]},{"cell_type":"code","source":"# text = \"29 m with anorexia nervosa hasnt dealt with the reality of i have an e EOS day EOS who has a hard time opening up about itim in a mental block of how to communicate with my boss who has had my back since hire have communicated that i have a e EOS day 4 month s ago that my e EOS day has to put it lightly hit a spike if i dont quit im scared ill be in inpatient again EOS i feel guilty due to the timing of me going on vacation for january my boss approved it but i also havent had a proper meal in month s due to the mentality this job puts me in because of the location im at EOS i work tomorrow until tues day straight but i dont think if i continue with this job my body will let me im a little a scared im close to inpatient again but i cant financially handle that again im currently paying off a 2000 debt\"\n# query_text = \"high protein food, food contain high protein\"\n\n# prompt_content = (\n#     f\"Does this paragraph '{text}' mention '{query_text}'? \"\n#     \"Return answer in format: [yes/no], phrases related to \"\n#     f\"{query_text} is [...]\"\n# )\n\n\n# response = query_model(\n#     system_message,\n#     user_message=prompt_content,\n#     temperature=0.1,\n#     max_length=50)\n# display(Markdown(colorize_text(f\"{response}\")))\n","metadata":{"execution":{"iopub.status.busy":"2024-10-17T03:36:25.231487Z","iopub.execute_input":"2024-10-17T03:36:25.231976Z","iopub.status.idle":"2024-10-17T03:36:28.749664Z","shell.execute_reply.started":"2024-10-17T03:36:25.231931Z","shell.execute_reply":"2024-10-17T03:36:28.748652Z"},"trusted":true},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"\n\n**<font color='red'>Question:</font>** Does this sentence '29 m with anorexia nervosa hasnt dealt with the reality of i have an e EOS day EOS who has a hard time opening up about itim in a mental block of how to communicate with my boss who has had my back since hire have communicated that i have a e EOS day 4 month s ago that my e EOS day has to put it lightly hit a spike if i dont quit im scared ill be in inpatient again EOS i feel guilty due to the timing of me going on vacation for january my boss approved it but i also havent had a proper meal in month s due to the mentality this job puts me in because of the location im at EOS i work tomorrow until tues day straight but i dont think if i continue with this job my body will let me im a little a scared im close to inpatient again but i cant financially handle that again im currently paying off a 2000 debt' mention 'high protein food, food contain high protein'? Return answer in format: [yes/no], phrases related to 29 m with anorexia nervosa hasnt dealt with the reality of i have an e EOS day EOS who has a hard time opening up about itim in a mental block of how to communicate with my boss who has had my back since hire have communicated that i have a e EOS day 4 month s ago that my e EOS day has to put it lightly hit a spike if i dont quit im scared ill be in inpatient again EOS i feel guilty due to the timing of me going on vacation for january my boss approved it but i also havent had a proper meal in month s due to the mentality this job puts me in because of the location im at EOS i work tomorrow until tues day straight but i dont think if i continue with this job my body will let me im a little a scared im close to inpatient again but i cant financially handle that again im currently paying off a 2000 debt is [...] \n\n**<font color='green'>Answer:</font>** [no] \n\n**<font color='magenta'>Total time:</font>** 3.51 sec."},"metadata":{}}]},{"cell_type":"code","source":"# query_text = \"29 m with anorexia nervosa hasnt dealt with the reality of i have an e EOS day EOS who has a hard time opening up about itim in a mental block of how to communicate with my boss who has had my back since hire have communicated that i have a e EOS day 4 month s ago that my e EOS day has to put it lightly hit a spike if i dont quit im scared ill be in inpatient again EOS i feel guilty due to the timing of me going on vacation for january my boss approved it but i also havent had a proper meal in month s due to the mentality this job puts me in because of the location im at EOS i work tomorrow until tues day straight but i dont think if i continue with this job my body will let me im a little a scared im close to inpatient again but i cant financially handle that again im currently paying off a 2000 debt\"\n# anchor_topic_text = \"eating disorder, binging, purging, recovery, treatment, anorexia nervosa, anorexic, bulimia, bulimic, binge eating disorders, arfid, osfed, pica\"\n\n# prompt_content = (\n#     f\"Does this sentence '{query_text}' mention '{anchor_topic_text}'? \"\n#     \"Return answer in format: [yes/no], phrases related to \"\n#     f\"{query_text} is [...]\"\n# )\n\n\n# response = query_model(\n#     system_message,\n#     user_message=prompt_content,\n#     temperature=0.1,\n#     max_length=50)\n# display(Markdown(colorize_text(f\"{response}\")))\n","metadata":{"execution":{"iopub.status.busy":"2024-10-17T03:38:26.165355Z","iopub.execute_input":"2024-10-17T03:38:26.166012Z","iopub.status.idle":"2024-10-17T03:38:57.970775Z","shell.execute_reply.started":"2024-10-17T03:38:26.165971Z","shell.execute_reply":"2024-10-17T03:38:57.969872Z"},"trusted":true},"execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"\n\n**<font color='red'>Question:</font>** Does this sentence '29 m with anorexia nervosa hasnt dealt with the reality of i have an e EOS day EOS who has a hard time opening up about itim in a mental block of how to communicate with my boss who has had my back since hire have communicated that i have a e EOS day 4 month s ago that my e EOS day has to put it lightly hit a spike if i dont quit im scared ill be in inpatient again EOS i feel guilty due to the timing of me going on vacation for january my boss approved it but i also havent had a proper meal in month s due to the mentality this job puts me in because of the location im at EOS i work tomorrow until tues day straight but i dont think if i continue with this job my body will let me im a little a scared im close to inpatient again but i cant financially handle that again im currently paying off a 2000 debt' mention 'eating disorder, binging, purging, recovery, treatment and recovery of anorexia nervosa, anorexic, bulimia, bulimic, binge eating disorders, arfid, osfed, pica'? Return answer in format: [yes/no], phrases related to 29 m with anorexia nervosa hasnt dealt with the reality of i have an e EOS day EOS who has a hard time opening up about itim in a mental block of how to communicate with my boss who has had my back since hire have communicated that i have a e EOS day 4 month s ago that my e EOS day has to put it lightly hit a spike if i dont quit im scared ill be in inpatient again EOS i feel guilty due to the timing of me going on vacation for january my boss approved it but i also havent had a proper meal in month s due to the mentality this job puts me in because of the location im at EOS i work tomorrow until tues day straight but i dont think if i continue with this job my body will let me im a little a scared im close to inpatient again but i cant financially handle that again im currently paying off a 2000 debt is [...] \n\n**<font color='green'>Answer:</font>** [yes]\n\nPhrases related to eating disorders mentioned: \n- anorexia nervosa\n- EOS day (likely referring to an eating disorder episode)\n- anorexic\n- bulimia (implied by \"purging\")\n- bul \n\n**<font color='magenta'>Total time:</font>** 31.8 sec."},"metadata":{}}]},{"cell_type":"markdown","source":"# load data and try examples","metadata":{}},{"cell_type":"code","source":"text_df = pd.read_csv(\"/kaggle/input/eddata/sm_eos.csv\")\nquery_df = pd.read_csv(\"/kaggle/input/eddata/query_df.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-10-17T14:01:18.649409Z","iopub.execute_input":"2024-10-17T14:01:18.649790Z","iopub.status.idle":"2024-10-17T14:01:21.920815Z","shell.execute_reply.started":"2024-10-17T14:01:18.649753Z","shell.execute_reply":"2024-10-17T14:01:21.919563Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def format_prompt(text):\n    question_content = \"Does the paragraph mention any of the following topics:\\n\"\n    for i in range(len(query_df)):\n        question_content += f\"  ({i+1}) {query_df.topic[i]}: {query_df.description[i]}.\\n\"\n    answer_content = \"Return answer in format:\\n\"\n    for i in range(len(query_df)):\n        answer_content += f\"  ({i+1}) {query_df.topic[i]}: [yes/no], related phrases if any: \\n\"\n    paragragh_content = f\"Paragraph: '{text}' \\n\"\n    user_message = question_content + answer_content + paragragh_content\n    #print(user_message)\n    \n    return user_message","metadata":{"execution":{"iopub.status.busy":"2024-10-17T14:01:25.028057Z","iopub.execute_input":"2024-10-17T14:01:25.028809Z","iopub.status.idle":"2024-10-17T14:01:25.034427Z","shell.execute_reply.started":"2024-10-17T14:01:25.028770Z","shell.execute_reply":"2024-10-17T14:01:25.033440Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"t1 = time()\nprompt_content = format_prompt(text_df.text_w_eos[0])\nresponse = query_model(\n    system_message,\n    user_message=prompt_content,\n    temperature=0.5,\n    max_length=512)\ndisplay(Markdown(colorize_text(f\"{response}\")))\n","metadata":{"execution":{"iopub.status.busy":"2024-10-17T14:01:27.614845Z","iopub.execute_input":"2024-10-17T14:01:27.615654Z","iopub.status.idle":"2024-10-17T14:03:19.647079Z","shell.execute_reply.started":"2024-10-17T14:01:27.615614Z","shell.execute_reply":"2024-10-17T14:03:19.646099Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"\n\n**<font color='red'>Question:</font>** Does the paragraph mention any of the following topics:\n  (1) relation: Family and social relationships.\n  (2) protein: High protein food or diet.\n  (3) thinspo: Drive for thinness, want to be skinny or underweight (thinspiration).\n  (4) health: Physical or mental health issues.\n  (5) body: Body dissatisfaction.\n  (6) ed: Eating disorders(ED) diagnosis or recovery, ED includes anorexia nervosa, anorexic, bulimia, bulimic, binge eating disorders, arfid, osfed, pica.\n  (7) exercise: Physical exercise.\n  (8) meal: Routine of meals.\n  (9) crave: Craving for high calorie food or carbs.\n  (10) restrict: Restrict nutrition or calorie intake.\n  (11) binge: Binge eating.\n  (12) loss: Weight loss.\n  (13) gain: Weight gain.\n  (14) fear: Fear of weight gain.\n  (15) calorie: Count calorie.\nReturn answer in format:\n  (1) relation: [yes/no], related phrases if any: \n  (2) protein: [yes/no], related phrases if any: \n  (3) thinspo: [yes/no], related phrases if any: \n  (4) health: [yes/no], related phrases if any: \n  (5) body: [yes/no], related phrases if any: \n  (6) ed: [yes/no], related phrases if any: \n  (7) exercise: [yes/no], related phrases if any: \n  (8) meal: [yes/no], related phrases if any: \n  (9) crave: [yes/no], related phrases if any: \n  (10) restrict: [yes/no], related phrases if any: \n  (11) binge: [yes/no], related phrases if any: \n  (12) loss: [yes/no], related phrases if any: \n  (13) gain: [yes/no], related phrases if any: \n  (14) fear: [yes/no], related phrases if any: \n  (15) calorie: [yes/no], related phrases if any: \nParagraph: '20 pound down after 22 week s and 17 week s out from my first wellness show. check the posing progress too starting fat burners and cardio again this week after being sick so excited to see what comes on show day. ' \n \n\n**<font color='green'>Answer:</font>** (1) relation: no, \n(2) protein: no, \n(3) thinspo: yes, related phrases if any: '20 pound down', 'fat burners', \n(4) health: yes, related phrases if any: 'being sick', 'wellness show', \n(5) body: yes, related phrases if any: 'posing progress', \n(6) ed: no, \n(7) exercise: yes, related phrases if any: 'cardio', \n(8) meal: no, \n(9) crave: no, \n(10) restrict: no, \n(11) binge: no, \n(12) loss: yes, related phrases if any: '20 pound down', \n(13) gain: no, \n(14) fear: no, \n(15) calorie: no \n\n**<font color='magenta'>Total time:</font>** 112.02 sec."},"metadata":{}}]},{"cell_type":"code","source":"\nprompt_content = format_prompt(text_df.text_w_eos[1])\nresponse = query_model(\n    system_message,\n    user_message=prompt_content,\n    temperature=0,\n    max_length=512)\ndisplay(Markdown(colorize_text(f\"{response}\")))\n","metadata":{"execution":{"iopub.status.busy":"2024-10-17T14:05:35.698916Z","iopub.execute_input":"2024-10-17T14:05:35.699789Z","iopub.status.idle":"2024-10-17T14:09:30.003761Z","shell.execute_reply.started":"2024-10-17T14:05:35.699745Z","shell.execute_reply":"2024-10-17T14:09:30.002843Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"\n\n**<font color='red'>Question:</font>** Does the paragraph mention any of the following topics:\n  (1) relation: Family and social relationships.\n  (2) protein: High protein food or diet.\n  (3) thinspo: Drive for thinness, want to be skinny or underweight (thinspiration).\n  (4) health: Physical or mental health issues.\n  (5) body: Body dissatisfaction.\n  (6) ed: Eating disorders(ED) diagnosis or recovery, ED includes anorexia nervosa, anorexic, bulimia, bulimic, binge eating disorders, arfid, osfed, pica.\n  (7) exercise: Physical exercise.\n  (8) meal: Routine of meals.\n  (9) crave: Craving for high calorie food or carbs.\n  (10) restrict: Restrict nutrition or calorie intake.\n  (11) binge: Binge eating.\n  (12) loss: Weight loss.\n  (13) gain: Weight gain.\n  (14) fear: Fear of weight gain.\n  (15) calorie: Count calorie.\nReturn answer in format:\n  (1) relation: [yes/no], related phrases if any: \n  (2) protein: [yes/no], related phrases if any: \n  (3) thinspo: [yes/no], related phrases if any: \n  (4) health: [yes/no], related phrases if any: \n  (5) body: [yes/no], related phrases if any: \n  (6) ed: [yes/no], related phrases if any: \n  (7) exercise: [yes/no], related phrases if any: \n  (8) meal: [yes/no], related phrases if any: \n  (9) crave: [yes/no], related phrases if any: \n  (10) restrict: [yes/no], related phrases if any: \n  (11) binge: [yes/no], related phrases if any: \n  (12) loss: [yes/no], related phrases if any: \n  (13) gain: [yes/no], related phrases if any: \n  (14) fear: [yes/no], related phrases if any: \n  (15) calorie: [yes/no], related phrases if any: \nParagraph: 'tw ana body dysmorphia describing body potentially triggering adjectives to describe body calorie s intakecounting nonrecoveryall of my friends my bf and my bfs family tell me im skinny. and im not. i dont have a flat stomach. i have a slight double chin. my legs arent thin and my arms have fat on them. im trying to lose 35 pounds in 6 month s. i originally wanted to lose that much in about 34 month s but pushed it back farther because my boyfriend was afraid i was going to die. im not going to. im eating abt 1400 calorie s a day. and he recently did research and told me that that is pretty average for weight loss. and that triggered me and made me feel like i wasnt doing enough. he forces me to eat every time im with him. even in front of his family and it makes me so uncomfortable. it makes me partially dread seeing him bcuz ik he will end up making me eat something. ik i set a goal to eat around 1400 calorie s but i want to eat nothing. my bf just doesnt understand how it feels to have ana. he tells me abt his concerns for me and i keep starving myself and he gets hurt that i wont take his feelings into consideration. but he doesnt understand that i cant just change. bcuz the thing is that i dont want to get better and i wont want to recover until im skinny. ' \n \n\n**<font color='green'>Answer:</font>** (1) relation: yes, related phrases if any:'my friends','my bf','my bfs family','my boyfriend'\n(2) protein: no\n(3) thinspo: yes, related phrases if any: 'im skinny', 'im not', 'i want to be skinny', 'until im skinny'\n(4) health: yes, related phrases if any: 'im going to die', 'ana', 'bcuz the thing is that i dont want to get better and i wont want to recover'\n(5) body: yes, related phrases if any: 'body dysmorphia','my body','my legs arent thin and my arms have fat on them','my stomach', 'double chin'\n(6) ed: yes, related phrases if any: 'ana', 'ana body dysmorphia', 'i keep starving myself', 'i wont want to recover'\n(7) exercise: no\n(8) meal: yes, related phrases if any: 'i want to eat nothing', 'i set a goal to eat around 1400 calorie s', 'he forces me to eat every time im with him'\n(9) crave: no\n(10) restrict: yes, related phrases if any: 'i want to eat nothing', 'i set a goal to eat around 1400 calorie s'\n(11) binge: no\n(12) loss: yes, related phrases if any: 'im trying to lose 35 pounds in 6 month s'\n(13) gain: no\n(14) fear: yes, related phrases if any: 'fear of weight gain', 'fear of not being skinny'\n(15) calorie: yes, related phrases if any: '1400 calorie s a day', 'i set a goal to eat around 1400 calorie s' \n\n**<font color='magenta'>Total time:</font>** 234.3 sec."},"metadata":{}}]},{"cell_type":"code","source":"# # sequential processing \n\n# answer_df = text_df[['sm_id', 'text_w_eos']].copy()\n# # Create a new column 'answer_string'\n# answer_df['answer_string'] = \"\"\n\n\n# for i in text_df.index:\n#     prompt_content = format_prompt(text_df.text_w_eos[i])\n#     response = query_model(\n#         system_message,\n#         user_message=prompt_content,\n#         temperature=0.1,\n#         max_length=512)\n#     display(Markdown(colorize_text(f\"{response}\")))\n#     # Check if \"Answer:\" is in the response to avoid errors\n#     if \"Answer:\" in response:\n#         answer = response.split(\"Answer:\")[1]\n#         answer_df.loc[i, 'answer_string'] = answer\n#     else:\n#         answer_df.loc[i, 'answer_string'] = \"Answer not found\"\n\n# answer_df.to_csv('/kaggle/working/answer_df.csv', index=False)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-17T14:47:54.936760Z","iopub.execute_input":"2024-10-17T14:47:54.937679Z","iopub.status.idle":"2024-10-17T14:47:54.942041Z","shell.execute_reply.started":"2024-10-17T14:47:54.937635Z","shell.execute_reply":"2024-10-17T14:47:54.941145Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"# batch processing","metadata":{}},{"cell_type":"code","source":"def query_model_batch(\n        system_message,\n        user_messages,\n        temperature=0,\n        max_length=1024\n    ):\n    start_time = time()\n    # Add \"Question: ... Answer:\" to each user message for clarity\n    batched_messages = [\n        \"Question: \" + message + \" Answer:\" for message in user_messages\n    ]\n    \n    # Construct prompts for each message in batch\n    all_prompts = [\n        pipe.tokenizer.apply_chat_template(\n            [\n                {\"role\": \"system\", \"content\": system_message},\n                {\"role\": \"user\", \"content\": user_message}\n            ],\n            tokenize=False,\n            add_generation_prompt=True\n        ) for user_message in batched_messages\n    ]\n    \n    # Define the end-of-sequence terminators\n    terminators = [\n        pipe.tokenizer.eos_token_id,\n        pipe.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n    ]\n    \n    # Run the batch inference\n    sequences = pipe(\n        all_prompts,\n        do_sample=True,\n        top_p=0.5,\n        temperature=temperature,\n        num_return_sequences=1,\n        eos_token_id=terminators,\n        max_new_tokens=max_length,\n        return_full_text=False,\n        pad_token_id=terminators[0]\n    )\n    \n    # Extract generated text for each sequence\n    answers = []\n    for i, sequence in enumerate(sequences):\n        answer = sequence[0]['generated_text']\n        total_time = f\"Total time: {round(time() - start_time, 2)} sec.\"\n        # Format the response with timing information\n        answers.append(batched_messages[i] + \" \" + answer + \" \" + total_time)\n\n    return answers\n","metadata":{"execution":{"iopub.status.busy":"2024-10-17T15:03:51.061972Z","iopub.execute_input":"2024-10-17T15:03:51.063257Z","iopub.status.idle":"2024-10-17T15:03:51.073715Z","shell.execute_reply.started":"2024-10-17T15:03:51.063187Z","shell.execute_reply":"2024-10-17T15:03:51.072539Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"\nbatch_size = 10\n\n# Load the previous answer DataFrame\nanswer_df = pd.read_csv(\"/kaggle/working/answer_df.csv\")\n\n# Filter for rows where 'answer_string' is NaN\nunanswered_df = answer_df[answer_df['answer_string'].isna()]\n\n# Get the indices of these NaN entries in the original DataFrame\nindices_to_update = unanswered_df.index[:batch_size]\n\n# Prepare prompt content for the first 10 entries with NaN answer_string\nprompts = [format_prompt(text) for text in unanswered_df['text_w_eos'].iloc[:batch_size]]\n\n# Save the indices list if needed for later use\nindices_to_update_list = list(indices_to_update)\nindices_to_update_list\n\n\n# Batch process all prompts at once\nresponses = query_model_batch(\n    system_message=system_message,\n    user_messages=prompts,\n    temperature=0.1,\n    max_length=512\n)\n\n\n# Display and process responses in a loop\nfor i, response in enumerate(responses):\n    #display(Markdown(colorize_text(f\"{response}\")))\n    \n    # Extract answer if available\n    if \"Answer:\" in response:\n        answer = response.split(\"Answer:\")[1]\n        # Use the original index from indices_to_update_list\n        answer_df.loc[indices_to_update_list[i], 'answer_string'] = answer\n    else:\n        # Use the original index from indices_to_update_list\n        answer_df.loc[indices_to_update_list[i], 'answer_string'] = \"Answer not found\"\n\n#answer_df.to_csv('/kaggle/working/answer_df.csv', index=False)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-17T15:10:47.741252Z","iopub.execute_input":"2024-10-17T15:10:47.742122Z","iopub.status.idle":"2024-10-17T15:16:02.197872Z","shell.execute_reply.started":"2024-10-17T15:10:47.742081Z","shell.execute_reply":"2024-10-17T15:16:02.196364Z"},"trusted":true},"execution_count":46,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[46], line 21\u001b[0m\n\u001b[1;32m     17\u001b[0m indices_to_update_list\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Batch process all prompts at once\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m responses \u001b[38;5;241m=\u001b[39m \u001b[43mquery_model_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43msystem_message\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msystem_message\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43muser_messages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\n\u001b[1;32m     26\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Display and process responses in a loop\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, response \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(responses):\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;66;03m#display(Markdown(colorize_text(f\"{response}\")))\u001b[39;00m\n\u001b[1;32m     32\u001b[0m     \n\u001b[1;32m     33\u001b[0m     \u001b[38;5;66;03m# Extract answer if available\u001b[39;00m\n","Cell \u001b[0;32mIn[39], line 32\u001b[0m, in \u001b[0;36mquery_model_batch\u001b[0;34m(system_message, user_messages, temperature, max_length)\u001b[0m\n\u001b[1;32m     26\u001b[0m terminators \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     27\u001b[0m     pipe\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39meos_token_id,\n\u001b[1;32m     28\u001b[0m     pipe\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mconvert_tokens_to_ids(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<|eot_id|>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     29\u001b[0m ]\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Run the batch inference\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m sequences \u001b[38;5;241m=\u001b[39m \u001b[43mpipe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mall_prompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_return_sequences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mterminators\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_full_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mterminators\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Extract generated text for each sequence\u001b[39;00m\n\u001b[1;32m     45\u001b[0m answers \u001b[38;5;241m=\u001b[39m []\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_generation.py:272\u001b[0m, in \u001b[0;36mTextGenerationPipeline.__call__\u001b[0;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    270\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(chats, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 272\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtext_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py:1249\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1245\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m can_use_iterator:\n\u001b[1;32m   1246\u001b[0m     final_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[1;32m   1247\u001b[0m         inputs, num_workers, batch_size, preprocess_params, forward_params, postprocess_params\n\u001b[1;32m   1248\u001b[0m     )\n\u001b[0;32m-> 1249\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfinal_iterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1250\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n\u001b[1;32m   1251\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/pipelines/pt_utils.py:124\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_item()\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer(item, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/pipelines/pt_utils.py:125\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[1;32m    124\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterator)\n\u001b[0;32m--> 125\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;66;03m# Try to infer the size of the batch\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py:1175\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1173\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1174\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m-> 1175\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1176\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m   1177\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_generation.py:370\u001b[0m, in \u001b[0;36mTextGenerationPipeline._forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    367\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration_config\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m generate_kwargs:\n\u001b[1;32m    368\u001b[0m     generate_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration_config\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeneration_config\n\u001b[0;32m--> 370\u001b[0m generated_sequence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    371\u001b[0m out_b \u001b[38;5;241m=\u001b[39m generated_sequence\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    372\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:2048\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2040\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2041\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2042\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2043\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2044\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2045\u001b[0m     )\n\u001b[1;32m   2047\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2048\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2049\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2050\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2051\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2052\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2053\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2054\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2055\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2056\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2058\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2059\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   2060\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   2061\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   2062\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2067\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   2068\u001b[0m     )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:3018\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3015\u001b[0m next_token_logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits\u001b[38;5;241m.\u001b[39mclone()[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m   3017\u001b[0m \u001b[38;5;66;03m# pre-process distribution\u001b[39;00m\n\u001b[0;32m-> 3018\u001b[0m next_token_scores \u001b[38;5;241m=\u001b[39m \u001b[43mlogits_processor\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_token_logits\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3020\u001b[0m \u001b[38;5;66;03m# Store scores, attentions and hidden_states when required\u001b[39;00m\n\u001b[1;32m   3021\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_dict_in_generate:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/generation/logits_process.py:104\u001b[0m, in \u001b[0;36mLogitsProcessorList.__call__\u001b[0;34m(self, input_ids, scores, **kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m         scores \u001b[38;5;241m=\u001b[39m processor(input_ids, scores, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 104\u001b[0m         scores \u001b[38;5;241m=\u001b[39m \u001b[43mprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscores\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m scores\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/generation/logits_process.py:480\u001b[0m, in \u001b[0;36mTopPLogitsWarper.__call__\u001b[0;34m(self, input_ids, scores)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;66;03m# scatter sorted tensors to original indexing\u001b[39;00m\n\u001b[1;32m    479\u001b[0m indices_to_remove \u001b[38;5;241m=\u001b[39m sorted_indices_to_remove\u001b[38;5;241m.\u001b[39mscatter(\u001b[38;5;241m1\u001b[39m, sorted_indices, sorted_indices_to_remove)\n\u001b[0;32m--> 480\u001b[0m scores_processed \u001b[38;5;241m=\u001b[39m \u001b[43mscores\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmasked_fill\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindices_to_remove\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilter_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m scores_processed\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"answer_df.head(20)","metadata":{"execution":{"iopub.status.busy":"2024-10-17T15:09:11.286698Z","iopub.execute_input":"2024-10-17T15:09:11.287402Z","iopub.status.idle":"2024-10-17T15:09:11.299008Z","shell.execute_reply.started":"2024-10-17T15:09:11.287360Z","shell.execute_reply":"2024-10-17T15:09:11.298045Z"},"trusted":true},"execution_count":44,"outputs":[{"execution_count":44,"output_type":"execute_result","data":{"text/plain":"      sm_id                                         text_w_eos  \\\n0   10002yt  20 pound down after 22 week s and 17 week s ou...   \n1   1003i3b  tw ana body dysmorphia describing body potenti...   \n2   1003xw2  hello fellow brawearing friends. this is going...   \n3   10042jf  according to cronometer 100 gram of canola oil...   \n4   1004j21  its been two week s since i started strength t...   \n5   1008hih  i made panfried chicken thighs ovenroasted pot...   \n6   100ash9  i have aversive arfid so i have a very bad fea...   \n7   100aza8  original post here httpswww. reddit. comrcicoc...   \n8   100bwe6  for context i am early 30s attending a corpora...   \n9   100c6y2  i got this from a thrift store and was pleasan...   \n10  100cs6r   update 41523hey folks. so.... it turns out th...   \n11  100ejqe  so it ended up pretty great. i ended up cuttin...   \n12  100emdr  i feel weak just reading this. bbc news marath...   \n13  100f1e5  happy new year s from one frugalmalefashion ju...   \n14  100hdao  this is the thread for week ly questions and s...   \n15  100hpr3  c210k wont let me use the app without allowing...   \n16  100ncvi  after two kids and gaining over 70lbs i am fin...   \n17  100nlca  i used to feel bad about having a small facean...   \n18  100nn9c  hi. we eat a lot of grilled chicken and i was ...   \n19  100ohc1   hi first of all thank you for your help with ...   \n\n                                        answer_string  \n0    (1) relation: no, \\n(2) protein: no, \\n(3) th...  \n1    (1) relation: yes, related phrases if any:'my...  \n2    (1) relation: yes, related phrases if any: fa...  \n3    (1) relation: no, \\n(2) protein: no, \\n(3) th...  \n4    (1) relation: no, \\n(2) protein: no, \\n(3) th...  \n5    (1) relation: no, \\n(2) protein: yes, related...  \n6    (1) relation: no, \\n(2) protein: no, \\n(3) th...  \n7    (1) relation: yes, related phrases if any:'my...  \n8    (1) relation: no, \\n(2) protein: no, \\n(3) th...  \n9    (1) relation: no, \\n(2) protein: no, \\n(3) th...  \n10   (1) relation: no, \\n(2) protein: no, \\n(3) th...  \n11                                                NaN  \n12                                                NaN  \n13                                                NaN  \n14                                                NaN  \n15                                                NaN  \n16                                                NaN  \n17                                                NaN  \n18                                                NaN  \n19                                                NaN  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sm_id</th>\n      <th>text_w_eos</th>\n      <th>answer_string</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>10002yt</td>\n      <td>20 pound down after 22 week s and 17 week s ou...</td>\n      <td>(1) relation: no, \\n(2) protein: no, \\n(3) th...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1003i3b</td>\n      <td>tw ana body dysmorphia describing body potenti...</td>\n      <td>(1) relation: yes, related phrases if any:'my...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1003xw2</td>\n      <td>hello fellow brawearing friends. this is going...</td>\n      <td>(1) relation: yes, related phrases if any: fa...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>10042jf</td>\n      <td>according to cronometer 100 gram of canola oil...</td>\n      <td>(1) relation: no, \\n(2) protein: no, \\n(3) th...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1004j21</td>\n      <td>its been two week s since i started strength t...</td>\n      <td>(1) relation: no, \\n(2) protein: no, \\n(3) th...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>1008hih</td>\n      <td>i made panfried chicken thighs ovenroasted pot...</td>\n      <td>(1) relation: no, \\n(2) protein: yes, related...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>100ash9</td>\n      <td>i have aversive arfid so i have a very bad fea...</td>\n      <td>(1) relation: no, \\n(2) protein: no, \\n(3) th...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>100aza8</td>\n      <td>original post here httpswww. reddit. comrcicoc...</td>\n      <td>(1) relation: yes, related phrases if any:'my...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>100bwe6</td>\n      <td>for context i am early 30s attending a corpora...</td>\n      <td>(1) relation: no, \\n(2) protein: no, \\n(3) th...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>100c6y2</td>\n      <td>i got this from a thrift store and was pleasan...</td>\n      <td>(1) relation: no, \\n(2) protein: no, \\n(3) th...</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>100cs6r</td>\n      <td>update 41523hey folks. so.... it turns out th...</td>\n      <td>(1) relation: no, \\n(2) protein: no, \\n(3) th...</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>100ejqe</td>\n      <td>so it ended up pretty great. i ended up cuttin...</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>100emdr</td>\n      <td>i feel weak just reading this. bbc news marath...</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>100f1e5</td>\n      <td>happy new year s from one frugalmalefashion ju...</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>100hdao</td>\n      <td>this is the thread for week ly questions and s...</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>100hpr3</td>\n      <td>c210k wont let me use the app without allowing...</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>100ncvi</td>\n      <td>after two kids and gaining over 70lbs i am fin...</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>100nlca</td>\n      <td>i used to feel bad about having a small facean...</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>100nn9c</td>\n      <td>hi. we eat a lot of grilled chicken and i was ...</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>100ohc1</td>\n      <td>hi first of all thank you for your help with ...</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"answer_df.to_csv('/kaggle/working/answer_df.csv', index=False)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-17T15:09:15.518414Z","iopub.execute_input":"2024-10-17T15:09:15.519036Z","iopub.status.idle":"2024-10-17T15:09:18.534677Z","shell.execute_reply.started":"2024-10-17T15:09:15.518996Z","shell.execute_reply":"2024-10-17T15:09:18.533600Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}