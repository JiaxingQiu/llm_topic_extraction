{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08f2e086",
   "metadata": {},
   "source": [
    "# gpt4omini / llama8binstruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9791a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55d6970b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip uninstall -y transformer-engine\n",
    "# %pip install torch==2.2.0\n",
    "# %pip install transformers==4.42.4\n",
    "# %pip install flash-attn==2.2.0\n",
    "# %pip install sentence-transformers==2.7.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b86b06b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cf3eba4ace943efa2e781abead9b33a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('nvidia/NV-Embed-v2', device='cuda' if torch.cuda.is_available() else 'cpu', trust_remote_code=True)\n",
    "model.max_seq_length = 128\n",
    "model.tokenizer.padding_side = \"right\"\n",
    "print(\"Model loaded on\", model.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fce4c497",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cba00735178448f8b5b9c0168b18e68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/997 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fa10b234b8847f39ab64d19362336bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b338ec1be23f4d1ba197a7f77bbd2354",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db1141719d354c4c942b8e6ed5699099",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/551 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "771f51e1e30343469d49cd5f89c51def",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f081f8d3d7e847d9836c4827590e6c50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/298 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load done\n",
      "Starting embedding\n",
      "[[87.97088623046875, -0.4589482247829437], [1.0263891220092773, 82.64778900146484]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Each query needs to be accompanied by an corresponding instruction describing the task.\n",
    "task_name_to_instruct = {\"example\": \"Given a question, retrieve passages that answer the question\",}\n",
    "\n",
    "query_prefix = \"Instruct: \"+task_name_to_instruct[\"example\"]+\"\\nQuery: \"\n",
    "queries = [\n",
    "    'are judo throws allowed in wrestling?', \n",
    "    'how to become a radiology technician in michigan?'\n",
    "    ]\n",
    "\n",
    "# No instruction needed for retrieval passages\n",
    "passages = [\n",
    "    \"Since you're reading this, you are probably someone from a judo background or someone who is just wondering how judo techniques can be applied under wrestling rules. So without further ado, let's get to the question. Are Judo throws allowed in wrestling? Yes, judo throws are allowed in freestyle and folkstyle wrestling. You only need to be careful to follow the slam rules when executing judo throws. In wrestling, a slam is lifting and returning an opponent to the mat with unnecessary force.\",\n",
    "    \"Below are the basic steps to becoming a radiologic technologist in Michigan:Earn a high school diploma. As with most careers in health care, a high school education is the first step to finding entry-level employment. Taking classes in math and science, such as anatomy, biology, chemistry, physiology, and physics, can help prepare students for their college studies and future careers.Earn an associate degree. Entry-level radiologic positions typically require at least an Associate of Applied Science. Before enrolling in one of these degree programs, students should make sure it has been properly accredited by the Joint Review Committee on Education in Radiologic Technology (JRCERT).Get licensed or certified in the state of Michigan.\"\n",
    "]\n",
    "\n",
    "# load model with tokenizer\n",
    "model = SentenceTransformer('nvidia/NV-Embed-v2', trust_remote_code=True)\n",
    "model.max_seq_length = 128\n",
    "model.tokenizer.padding_side=\"right\"\n",
    "print(\"Load done\")\n",
    "\n",
    "def add_eos(input_examples):\n",
    "    input_examples = [input_example + model.tokenizer.eos_token for input_example in input_examples]\n",
    "    return input_examples\n",
    "\n",
    "# get the embeddings\n",
    "batch_size = 2\n",
    "print(\"Starting embedding\")\n",
    "query_embeddings = model.encode(add_eos(queries), batch_size=batch_size, prompt=query_prefix, normalize_embeddings=True)\n",
    "passage_embeddings = model.encode(add_eos(passages), batch_size=batch_size, normalize_embeddings=True)\n",
    "\n",
    "scores = (query_embeddings @ passage_embeddings.T) * 100\n",
    "print(scores.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e97652",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4902e7ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76eef809e30648e4a31b145c896a0315",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/28.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95a2ea0b10d24faf8f8688c7ce184846",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7732419600d443c88318452daf57b2f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfdfb6d5806d4803ae5ce921476bb525",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59c2d652e93c43f29fa929efc75c6d2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc6d830736ad4531b8b9db3ab7c3ff3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/789M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'LatentAttentionConfig' object has no attribute '_attn_implementation_internal'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m mdl_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall-mpnet-base-v2\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     11\u001b[0m mdl_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnvidia/NV-Embed-v2\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 13\u001b[0m model \u001b[38;5;241m=\u001b[39m SentenceTransformer(mdl_name, trust_remote_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Function to generate embeddings\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_embeddings\u001b[39m(text):\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# Generate embeddings\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sentence_transformers/SentenceTransformer.py:197\u001b[0m, in \u001b[0;36mSentenceTransformer.__init__\u001b[0;34m(self, model_name_or_path, modules, device, prompts, default_prompt_name, cache_folder, trust_remote_code, revision, token, use_auth_token, truncate_dim)\u001b[0m\n\u001b[1;32m    194\u001b[0m         model_name_or_path \u001b[38;5;241m=\u001b[39m __MODEL_HUB_ORGANIZATION__ \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m model_name_or_path\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_sentence_transformer_model(model_name_or_path, token, cache_folder\u001b[38;5;241m=\u001b[39mcache_folder, revision\u001b[38;5;241m=\u001b[39mrevision):\n\u001b[0;32m--> 197\u001b[0m     modules \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_sbert_model(\n\u001b[1;32m    198\u001b[0m         model_name_or_path,\n\u001b[1;32m    199\u001b[0m         token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[1;32m    200\u001b[0m         cache_folder\u001b[38;5;241m=\u001b[39mcache_folder,\n\u001b[1;32m    201\u001b[0m         revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[1;32m    202\u001b[0m         trust_remote_code\u001b[38;5;241m=\u001b[39mtrust_remote_code,\n\u001b[1;32m    203\u001b[0m     )\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    205\u001b[0m     modules \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_auto_model(\n\u001b[1;32m    206\u001b[0m         model_name_or_path,\n\u001b[1;32m    207\u001b[0m         token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    210\u001b[0m         trust_remote_code\u001b[38;5;241m=\u001b[39mtrust_remote_code,\n\u001b[1;32m    211\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sentence_transformers/SentenceTransformer.py:1296\u001b[0m, in \u001b[0;36mSentenceTransformer._load_sbert_model\u001b[0;34m(self, model_name_or_path, token, cache_folder, revision, trust_remote_code)\u001b[0m\n\u001b[1;32m   1294\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1295\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizer_args\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m hub_kwargs\n\u001b[0;32m-> 1296\u001b[0m     module \u001b[38;5;241m=\u001b[39m Transformer(model_name_or_path, cache_dir\u001b[38;5;241m=\u001b[39mcache_folder, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1297\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1298\u001b[0m     \u001b[38;5;66;03m# Normalize does not require any files to be loaded\u001b[39;00m\n\u001b[1;32m   1299\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m module_class \u001b[38;5;241m==\u001b[39m Normalize:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sentence_transformers/models/Transformer.py:36\u001b[0m, in \u001b[0;36mTransformer.__init__\u001b[0;34m(self, model_name_or_path, max_seq_length, model_args, cache_dir, tokenizer_args, do_lower_case, tokenizer_name_or_path)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_lower_case \u001b[38;5;241m=\u001b[39m do_lower_case\n\u001b[1;32m     35\u001b[0m config \u001b[38;5;241m=\u001b[39m AutoConfig\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_args, cache_dir\u001b[38;5;241m=\u001b[39mcache_dir)\n\u001b[0;32m---> 36\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_model(model_name_or_path, config, cache_dir, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_args)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m     39\u001b[0m     tokenizer_name_or_path \u001b[38;5;28;01mif\u001b[39;00m tokenizer_name_or_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m model_name_or_path,\n\u001b[1;32m     40\u001b[0m     cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtokenizer_args,\n\u001b[1;32m     42\u001b[0m )\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# No max_seq_length set. Try to infer from model\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sentence_transformers/models/Transformer.py:65\u001b[0m, in \u001b[0;36mTransformer._load_model\u001b[0;34m(self, model_name_or_path, config, cache_dir, **model_args)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_mt5_model(model_name_or_path, config, cache_dir, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_args)\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_model \u001b[38;5;241m=\u001b[39m AutoModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m     66\u001b[0m         model_name_or_path, config\u001b[38;5;241m=\u001b[39mconfig, cache_dir\u001b[38;5;241m=\u001b[39mcache_dir, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_args\n\u001b[1;32m     67\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:559\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    557\u001b[0m     \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mregister(config\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, model_class, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    558\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m add_generation_mixin_to_remote_model(model_class)\n\u001b[0;32m--> 559\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m    560\u001b[0m         pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39mmodel_args, config\u001b[38;5;241m=\u001b[39mconfig, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    561\u001b[0m     )\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/modeling_utils.py:3886\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3880\u001b[0m config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_autoset_attn_implementation(\n\u001b[1;32m   3881\u001b[0m     config, use_flash_attention_2\u001b[38;5;241m=\u001b[39muse_flash_attention_2, torch_dtype\u001b[38;5;241m=\u001b[39mtorch_dtype, device_map\u001b[38;5;241m=\u001b[39mdevice_map\n\u001b[1;32m   3882\u001b[0m )\n\u001b[1;32m   3884\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ContextManagers(init_contexts):\n\u001b[1;32m   3885\u001b[0m     \u001b[38;5;66;03m# Let's make sure we don't run the init function of buffer modules\u001b[39;00m\n\u001b[0;32m-> 3886\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(config, \u001b[38;5;241m*\u001b[39mmodel_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   3888\u001b[0m \u001b[38;5;66;03m# make sure we use the model's config since the __init__ call might have copied it\u001b[39;00m\n\u001b[1;32m   3889\u001b[0m config \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/nvidia/NV-Embed-v2/7604d305b621f14095a1aa23d351674c2859553a/modeling_nvembed.py:323\u001b[0m, in \u001b[0;36mNVEmbedModel.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, config: NVEmbedConfig):\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(config)\n\u001b[0;32m--> 323\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlatent_attention_model \u001b[38;5;241m=\u001b[39m AutoModel\u001b[38;5;241m.\u001b[39mfrom_config(config\u001b[38;5;241m.\u001b[39mlatent_attention_config)\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_model \u001b[38;5;241m=\u001b[39m AutoModel\u001b[38;5;241m.\u001b[39mfrom_config(\n\u001b[1;32m    325\u001b[0m         config\u001b[38;5;241m.\u001b[39mtext_config,\n\u001b[1;32m    326\u001b[0m     ) \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mtext_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    327\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(config\u001b[38;5;241m.\u001b[39mtext_config\u001b[38;5;241m.\u001b[39m_name_or_path) \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mtext_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:440\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_config\u001b[0;34m(cls, config, **kwargs)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    439\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39m_from_config(config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    443\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    444\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    445\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/modeling_utils.py:1494\u001b[0m, in \u001b[0;36mPreTrainedModel._from_config\u001b[0;34m(cls, config, **kwargs)\u001b[0m\n\u001b[1;32m   1490\u001b[0m     dtype_orig \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_set_default_torch_dtype(torch_dtype)\n\u001b[1;32m   1492\u001b[0m config \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(config)  \u001b[38;5;66;03m# We do not want to modify the config inplace in _from_config.\u001b[39;00m\n\u001b[0;32m-> 1494\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39m_attn_implementation_internal \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1495\u001b[0m     \u001b[38;5;66;03m# In this case, the config has been created with the attn_implementation set by the user, which we\u001b[39;00m\n\u001b[1;32m   1496\u001b[0m     \u001b[38;5;66;03m# should respect.\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m     attn_implementation \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39m_attn_implementation_internal\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/configuration_utils.py:202\u001b[0m, in \u001b[0;36mPretrainedConfig.__getattribute__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattribute_map\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattribute_map\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    201\u001b[0m     key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattribute_map\u001b[39m\u001b[38;5;124m\"\u001b[39m)[key]\n\u001b[0;32m--> 202\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(key)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'LatentAttentionConfig' object has no attribute '_attn_implementation_internal'"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "# Load the pre-trained Sentence Transformer model\n",
    "mdl_name = 'all-mpnet-base-v2'\n",
    "mdl_name = \"nvidia/NV-Embed-v2\"\n",
    "\n",
    "model = SentenceTransformer(mdl_name, trust_remote_code=True)\n",
    "\n",
    "# Function to generate embeddings\n",
    "def get_embeddings(text):\n",
    "    # Generate embeddings\n",
    "    embedding = model.encode(text)\n",
    "    return embedding\n",
    "\n",
    "\n",
    "folder = \"llama_data\"\n",
    "res_df = pd.read_csv(\"../../\"+folder+\"/results.csv\")\n",
    "res_df = res_df.fillna(\"\")\n",
    "fea_df = pd.read_csv(\"../../data/fea_df.csv\")\n",
    "\n",
    "fea_df['description'] = fea_df['description'].apply(\n",
    "    lambda x: re.sub(r\"Must related to [^(]*\\(?.*?\\)?\\.\", \"\", x).strip()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "671e0b2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i wear baggy clothes to hide what i look like (0.3536);  people have noticed (0.1179); \n"
     ]
    }
   ],
   "source": [
    "# example\n",
    "\n",
    "# loop through each sm_id in res_df.sm_id\n",
    "# loop through each topic in fea_df.fea\n",
    "sm_id = \"104a474\"\n",
    "topic = \"bodyhate\"\n",
    "\n",
    "\n",
    "refer_str = fea_df.loc[fea_df.fea==topic, 'description'].tolist()[0]\n",
    "returned_str = res_df.loc[res_df.sm_id==sm_id, topic+\"_phrases\"].tolist()[0]\n",
    "returned_str_ls = returned_str.split(\";\") # split returned_str by \";\"[\"i wear baggy clothes to hide what i look like.\"]#\n",
    "res_str = \"\"\n",
    "for returned_str in returned_str_ls:\n",
    "    v1 = get_embeddings(refer_str)\n",
    "    v2 = get_embeddings(returned_str)\n",
    "    res_str = res_str + returned_str + \" (\" +str(round(cosine_similarity([v1], [v2])[0][0],4)) + \"); \"\n",
    "print(res_str)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5052ffb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Copy the DataFrame to store results\n",
    "# res_df_with_cos = res_df.copy()\n",
    "\n",
    "# # Loop through each sm_id with a progress bar\n",
    "# for sm_id in tqdm(res_df.sm_id.unique(), desc=\"Processing sm_ids\"):\n",
    "#     for topic in fea_df.fea.unique():\n",
    "        \n",
    "#         # Get reference string and returned string for the topic and sm_id\n",
    "#         refer_str = fea_df.loc[fea_df.fea == topic, 'description'].tolist()[0]\n",
    "#         returned_str = res_df.loc[res_df.sm_id == sm_id, f\"{topic}_phrases\"].tolist()[0]\n",
    "        \n",
    "#         if returned_str:  # Only process non-empty returned strings\n",
    "#             # Split returned_str by \";\"\n",
    "#             returned_str_ls = returned_str.split(\";\")\n",
    "\n",
    "#             # Initialize res_str to accumulate results\n",
    "#             res_str = \"\"\n",
    "\n",
    "#             # Loop through each phrase in returned_str_ls and compute similarity\n",
    "#             v1 = get_embeddings(refer_str)  # Compute embeddings for the reference string once\n",
    "#             for phrase in returned_str_ls:\n",
    "#                 v2 = get_embeddings(phrase)\n",
    "#                 similarity_score = round(cosine_similarity([v1], [v2])[0][0], 4)\n",
    "#                 res_str += f\"{phrase} ({similarity_score}); \"\n",
    "\n",
    "#             # Store the result in res_df_with_cos\n",
    "#             res_df_with_cos.loc[res_df_with_cos.sm_id == sm_id, f\"{topic}_phrases\"] = res_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d6d552f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import os\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Define the function to process each batch of res_df\n",
    "def process_batch(res_df_batch, fea_df):\n",
    "    res_df_with_cos_batch = res_df_batch.copy()\n",
    "    for sm_id in res_df_batch.sm_id.unique():\n",
    "        for topic in fea_df.fea.unique():\n",
    "            refer_str = fea_df.loc[fea_df.fea == topic, 'description'].tolist()[0]\n",
    "            returned_str = res_df_batch.loc[res_df_batch.sm_id == sm_id, f\"{topic}_phrases\"].tolist()[0]\n",
    "            \n",
    "            if returned_str:\n",
    "                returned_str_ls = returned_str.split(\";\")\n",
    "                res_str = \"\"\n",
    "                v1 = get_embeddings(refer_str)\n",
    "                for phrase in returned_str_ls:\n",
    "                    v2 = get_embeddings(phrase)\n",
    "                    similarity_score = round(cosine_similarity([v1], [v2])[0][0], 4)\n",
    "                    res_str += f\"{phrase} ({similarity_score}); \"\n",
    "                res_df_with_cos_batch.loc[res_df_with_cos_batch.sm_id == sm_id, f\"{topic}_phrases\"] = res_str\n",
    "    return res_df_with_cos_batch\n",
    "\n",
    "# Split res_df into 10 batches\n",
    "res_df_batches = np.array_split(res_df, 10)\n",
    "\n",
    "# Run each batch in parallel using joblib\n",
    "results = Parallel(n_jobs=10)(delayed(process_batch)(batch, fea_df) for batch in res_df_batches)\n",
    "\n",
    "# Combine results back into the main DataFrame\n",
    "res_df_with_cos = pd.concat(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b561ff7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sm_id</th>\n",
       "      <th>text_w_eos</th>\n",
       "      <th>answer_string</th>\n",
       "      <th>relation</th>\n",
       "      <th>protein</th>\n",
       "      <th>ed</th>\n",
       "      <th>exercise</th>\n",
       "      <th>meal</th>\n",
       "      <th>crave</th>\n",
       "      <th>restrict</th>\n",
       "      <th>...</th>\n",
       "      <th>gain_phrases</th>\n",
       "      <th>calorie_phrases</th>\n",
       "      <th>thinspo_phrases</th>\n",
       "      <th>leanbody_phrases</th>\n",
       "      <th>bodyhate_phrases</th>\n",
       "      <th>feargain_phrases</th>\n",
       "      <th>fearfood_phrases</th>\n",
       "      <th>fearcarb_phrases</th>\n",
       "      <th>nosocialeat_phrases</th>\n",
       "      <th>depressedmood_phrases</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1003i3b</td>\n",
       "      <td>tw ana body dysmorphia describing body potenti...</td>\n",
       "      <td>(1) relation: yes, related phrases if any:'my...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td>1400 calorie s a day (0.5580000281333923);  bo...</td>\n",
       "      <td>im skinny (0.5418000221252441);  lose 35 pound...</td>\n",
       "      <td>lose 35 pounds (0.29010000824928284);</td>\n",
       "      <td>i dont have a flat stomach (0.2605000138282776...</td>\n",
       "      <td>i wont want to recover until im skinny (0.3630...</td>\n",
       "      <td>i want to eat nothing (0.40639999508857727);  ...</td>\n",
       "      <td></td>\n",
       "      <td>it makes me so uncomfortable (0.22120000422000...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sm_id                                         text_w_eos  \\\n",
       "1  1003i3b  tw ana body dysmorphia describing body potenti...   \n",
       "\n",
       "                                       answer_string  relation protein   ed  \\\n",
       "1   (1) relation: yes, related phrases if any:'my...         1     0.0  1.0   \n",
       "\n",
       "  exercise meal crave restrict  ... gain_phrases  \\\n",
       "1      0.0  1.0   0.0      1.0  ...                \n",
       "\n",
       "                                     calorie_phrases  \\\n",
       "1  1400 calorie s a day (0.5580000281333923);  bo...   \n",
       "\n",
       "                                     thinspo_phrases  \\\n",
       "1  im skinny (0.5418000221252441);  lose 35 pound...   \n",
       "\n",
       "                         leanbody_phrases  \\\n",
       "1  lose 35 pounds (0.29010000824928284);    \n",
       "\n",
       "                                    bodyhate_phrases  \\\n",
       "1  i dont have a flat stomach (0.2605000138282776...   \n",
       "\n",
       "                                    feargain_phrases  \\\n",
       "1  i wont want to recover until im skinny (0.3630...   \n",
       "\n",
       "                                    fearfood_phrases fearcarb_phrases  \\\n",
       "1  i want to eat nothing (0.40639999508857727);  ...                    \n",
       "\n",
       "                                 nosocialeat_phrases depressedmood_phrases  \n",
       "1  it makes me so uncomfortable (0.22120000422000...                        \n",
       "\n",
       "[1 rows x 41 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_df_with_cos.loc[res_df_with_cos.sm_id==\"1003i3b\",:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3175b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df_with_cos.to_csv(\"../../\"+folder+\"/results_with_cos_\"+mdl_name+\".csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2f9872",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
