{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 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\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoModelForCausalLM, AutoTokenizer, pipeline\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Clear GPU cache\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "# Clear GPU cache\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "model_name = \"meta-llama/Llama-3.1-8B-Instruct\"  # Replace with exact path for 8B model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.15s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Check if CUDA is available\n",
    "device = 0 if torch.cuda.is_available() else -1  # Use GPU if available, otherwise fallback to CPU\n",
    "\n",
    "# Load the model with the appropriate device settings\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,                   \n",
    "    return_dict=True,\n",
    "    low_cpu_mem_usage=True,\n",
    "    torch_dtype=torch.float16,     # Use float16 for efficient memory usage on GPU\n",
    "    device_map=\"auto\" if device == 0 else None,  # Automatically distribute across devices or use CPU\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Initialize the text generation pipeline\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\" if device == 0 else None,  # Explicitly set device to GPU if available, else CPU\n",
    ")\n",
    "\n",
    "# # Example usage of the pipeline\n",
    "# output = pipe(\"Once upon a time,\", max_length=50)\n",
    "# print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.embed_tokens.weight is on cuda:0\n",
      "model.layers.0.self_attn.q_proj.weight is on cuda:0\n",
      "model.layers.0.self_attn.k_proj.weight is on cuda:0\n",
      "model.layers.0.self_attn.v_proj.weight is on cuda:0\n",
      "model.layers.0.self_attn.o_proj.weight is on cuda:0\n",
      "model.layers.0.mlp.gate_proj.weight is on cuda:0\n",
      "model.layers.0.mlp.up_proj.weight is on cuda:0\n",
      "model.layers.0.mlp.down_proj.weight is on cuda:0\n",
      "model.layers.0.input_layernorm.weight is on cuda:0\n",
      "model.layers.0.post_attention_layernorm.weight is on cuda:0\n",
      "model.layers.1.self_attn.q_proj.weight is on cuda:0\n",
      "model.layers.1.self_attn.k_proj.weight is on cuda:0\n",
      "model.layers.1.self_attn.v_proj.weight is on cuda:0\n",
      "model.layers.1.self_attn.o_proj.weight is on cuda:0\n",
      "model.layers.1.mlp.gate_proj.weight is on cuda:0\n",
      "model.layers.1.mlp.up_proj.weight is on cuda:0\n",
      "model.layers.1.mlp.down_proj.weight is on cuda:0\n",
      "model.layers.1.input_layernorm.weight is on cuda:0\n",
      "model.layers.1.post_attention_layernorm.weight is on cuda:0\n",
      "model.layers.2.self_attn.q_proj.weight is on cuda:0\n",
      "model.layers.2.self_attn.k_proj.weight is on cuda:0\n",
      "model.layers.2.self_attn.v_proj.weight is on cuda:0\n",
      "model.layers.2.self_attn.o_proj.weight is on cuda:0\n",
      "model.layers.2.mlp.gate_proj.weight is on cuda:0\n",
      "model.layers.2.mlp.up_proj.weight is on cuda:0\n",
      "model.layers.2.mlp.down_proj.weight is on cuda:0\n",
      "model.layers.2.input_layernorm.weight is on cuda:0\n",
      "model.layers.2.post_attention_layernorm.weight is on cuda:0\n",
      "model.layers.3.self_attn.q_proj.weight is on cuda:0\n",
      "model.layers.3.self_attn.k_proj.weight is on cuda:0\n",
      "model.layers.3.self_attn.v_proj.weight is on cuda:0\n",
      "model.layers.3.self_attn.o_proj.weight is on cuda:0\n",
      "model.layers.3.mlp.gate_proj.weight is on cuda:0\n",
      "model.layers.3.mlp.up_proj.weight is on cuda:0\n",
      "model.layers.3.mlp.down_proj.weight is on cuda:0\n",
      "model.layers.3.input_layernorm.weight is on cuda:0\n",
      "model.layers.3.post_attention_layernorm.weight is on cuda:0\n",
      "model.layers.4.self_attn.q_proj.weight is on cuda:0\n",
      "model.layers.4.self_attn.k_proj.weight is on cuda:0\n",
      "model.layers.4.self_attn.v_proj.weight is on cuda:0\n",
      "model.layers.4.self_attn.o_proj.weight is on cuda:0\n",
      "model.layers.4.mlp.gate_proj.weight is on cuda:0\n",
      "model.layers.4.mlp.up_proj.weight is on cuda:0\n",
      "model.layers.4.mlp.down_proj.weight is on cuda:0\n",
      "model.layers.4.input_layernorm.weight is on cuda:0\n",
      "model.layers.4.post_attention_layernorm.weight is on cuda:0\n",
      "model.layers.5.self_attn.q_proj.weight is on cuda:0\n",
      "model.layers.5.self_attn.k_proj.weight is on cuda:0\n",
      "model.layers.5.self_attn.v_proj.weight is on cuda:0\n",
      "model.layers.5.self_attn.o_proj.weight is on cuda:0\n",
      "model.layers.5.mlp.gate_proj.weight is on cuda:0\n",
      "model.layers.5.mlp.up_proj.weight is on cuda:0\n",
      "model.layers.5.mlp.down_proj.weight is on cuda:0\n",
      "model.layers.5.input_layernorm.weight is on cuda:0\n",
      "model.layers.5.post_attention_layernorm.weight is on cuda:0\n",
      "model.layers.6.self_attn.q_proj.weight is on cuda:0\n",
      "model.layers.6.self_attn.k_proj.weight is on cuda:0\n",
      "model.layers.6.self_attn.v_proj.weight is on cuda:0\n",
      "model.layers.6.self_attn.o_proj.weight is on cuda:0\n",
      "model.layers.6.mlp.gate_proj.weight is on cuda:0\n",
      "model.layers.6.mlp.up_proj.weight is on cuda:0\n",
      "model.layers.6.mlp.down_proj.weight is on cuda:0\n",
      "model.layers.6.input_layernorm.weight is on cuda:0\n",
      "model.layers.6.post_attention_layernorm.weight is on cuda:0\n",
      "model.layers.7.self_attn.q_proj.weight is on cuda:0\n",
      "model.layers.7.self_attn.k_proj.weight is on cuda:0\n",
      "model.layers.7.self_attn.v_proj.weight is on cuda:0\n",
      "model.layers.7.self_attn.o_proj.weight is on cuda:0\n",
      "model.layers.7.mlp.gate_proj.weight is on cuda:0\n",
      "model.layers.7.mlp.up_proj.weight is on cuda:0\n",
      "model.layers.7.mlp.down_proj.weight is on cuda:0\n",
      "model.layers.7.input_layernorm.weight is on cuda:0\n",
      "model.layers.7.post_attention_layernorm.weight is on cuda:0\n",
      "model.layers.8.self_attn.q_proj.weight is on cuda:0\n",
      "model.layers.8.self_attn.k_proj.weight is on cuda:0\n",
      "model.layers.8.self_attn.v_proj.weight is on cuda:0\n",
      "model.layers.8.self_attn.o_proj.weight is on cuda:0\n",
      "model.layers.8.mlp.gate_proj.weight is on cuda:0\n",
      "model.layers.8.mlp.up_proj.weight is on cuda:0\n",
      "model.layers.8.mlp.down_proj.weight is on cuda:0\n",
      "model.layers.8.input_layernorm.weight is on cuda:0\n",
      "model.layers.8.post_attention_layernorm.weight is on cuda:0\n",
      "model.layers.9.self_attn.q_proj.weight is on cuda:0\n",
      "model.layers.9.self_attn.k_proj.weight is on cuda:0\n",
      "model.layers.9.self_attn.v_proj.weight is on cuda:0\n",
      "model.layers.9.self_attn.o_proj.weight is on cuda:0\n",
      "model.layers.9.mlp.gate_proj.weight is on cuda:0\n",
      "model.layers.9.mlp.up_proj.weight is on cuda:0\n",
      "model.layers.9.mlp.down_proj.weight is on cuda:0\n",
      "model.layers.9.input_layernorm.weight is on cuda:0\n",
      "model.layers.9.post_attention_layernorm.weight is on cuda:0\n",
      "model.layers.10.self_attn.q_proj.weight is on cuda:0\n",
      "model.layers.10.self_attn.k_proj.weight is on cuda:0\n",
      "model.layers.10.self_attn.v_proj.weight is on cuda:0\n",
      "model.layers.10.self_attn.o_proj.weight is on cuda:0\n",
      "model.layers.10.mlp.gate_proj.weight is on cuda:0\n",
      "model.layers.10.mlp.up_proj.weight is on cuda:0\n",
      "model.layers.10.mlp.down_proj.weight is on cuda:0\n",
      "model.layers.10.input_layernorm.weight is on cuda:0\n",
      "model.layers.10.post_attention_layernorm.weight is on cuda:0\n",
      "model.layers.11.self_attn.q_proj.weight is on cuda:0\n",
      "model.layers.11.self_attn.k_proj.weight is on cuda:0\n",
      "model.layers.11.self_attn.v_proj.weight is on cuda:0\n",
      "model.layers.11.self_attn.o_proj.weight is on cuda:0\n",
      "model.layers.11.mlp.gate_proj.weight is on cuda:0\n",
      "model.layers.11.mlp.up_proj.weight is on cuda:0\n",
      "model.layers.11.mlp.down_proj.weight is on cuda:0\n",
      "model.layers.11.input_layernorm.weight is on cuda:0\n",
      "model.layers.11.post_attention_layernorm.weight is on cuda:0\n",
      "model.layers.12.self_attn.q_proj.weight is on cuda:0\n",
      "model.layers.12.self_attn.k_proj.weight is on cuda:0\n",
      "model.layers.12.self_attn.v_proj.weight is on cuda:0\n",
      "model.layers.12.self_attn.o_proj.weight is on cuda:0\n",
      "model.layers.12.mlp.gate_proj.weight is on cuda:0\n",
      "model.layers.12.mlp.up_proj.weight is on cuda:0\n",
      "model.layers.12.mlp.down_proj.weight is on cuda:0\n",
      "model.layers.12.input_layernorm.weight is on cuda:0\n",
      "model.layers.12.post_attention_layernorm.weight is on cuda:0\n",
      "model.layers.13.self_attn.q_proj.weight is on cuda:0\n",
      "model.layers.13.self_attn.k_proj.weight is on cuda:0\n",
      "model.layers.13.self_attn.v_proj.weight is on cuda:0\n",
      "model.layers.13.self_attn.o_proj.weight is on cuda:0\n",
      "model.layers.13.mlp.gate_proj.weight is on cuda:0\n",
      "model.layers.13.mlp.up_proj.weight is on cuda:0\n",
      "model.layers.13.mlp.down_proj.weight is on cuda:0\n",
      "model.layers.13.input_layernorm.weight is on cuda:0\n",
      "model.layers.13.post_attention_layernorm.weight is on cuda:0\n",
      "model.layers.14.self_attn.q_proj.weight is on cuda:1\n",
      "model.layers.14.self_attn.k_proj.weight is on cuda:1\n",
      "model.layers.14.self_attn.v_proj.weight is on cuda:1\n",
      "model.layers.14.self_attn.o_proj.weight is on cuda:1\n",
      "model.layers.14.mlp.gate_proj.weight is on cuda:1\n",
      "model.layers.14.mlp.up_proj.weight is on cuda:1\n",
      "model.layers.14.mlp.down_proj.weight is on cuda:1\n",
      "model.layers.14.input_layernorm.weight is on cuda:1\n",
      "model.layers.14.post_attention_layernorm.weight is on cuda:1\n",
      "model.layers.15.self_attn.q_proj.weight is on cuda:1\n",
      "model.layers.15.self_attn.k_proj.weight is on cuda:1\n",
      "model.layers.15.self_attn.v_proj.weight is on cuda:1\n",
      "model.layers.15.self_attn.o_proj.weight is on cuda:1\n",
      "model.layers.15.mlp.gate_proj.weight is on cuda:1\n",
      "model.layers.15.mlp.up_proj.weight is on cuda:1\n",
      "model.layers.15.mlp.down_proj.weight is on cuda:1\n",
      "model.layers.15.input_layernorm.weight is on cuda:1\n",
      "model.layers.15.post_attention_layernorm.weight is on cuda:1\n",
      "model.layers.16.self_attn.q_proj.weight is on cuda:1\n",
      "model.layers.16.self_attn.k_proj.weight is on cuda:1\n",
      "model.layers.16.self_attn.v_proj.weight is on cuda:1\n",
      "model.layers.16.self_attn.o_proj.weight is on cuda:1\n",
      "model.layers.16.mlp.gate_proj.weight is on cuda:1\n",
      "model.layers.16.mlp.up_proj.weight is on cuda:1\n",
      "model.layers.16.mlp.down_proj.weight is on cuda:1\n",
      "model.layers.16.input_layernorm.weight is on cuda:1\n",
      "model.layers.16.post_attention_layernorm.weight is on cuda:1\n",
      "model.layers.17.self_attn.q_proj.weight is on cuda:1\n",
      "model.layers.17.self_attn.k_proj.weight is on cuda:1\n",
      "model.layers.17.self_attn.v_proj.weight is on cuda:1\n",
      "model.layers.17.self_attn.o_proj.weight is on cuda:1\n",
      "model.layers.17.mlp.gate_proj.weight is on cuda:1\n",
      "model.layers.17.mlp.up_proj.weight is on cuda:1\n",
      "model.layers.17.mlp.down_proj.weight is on cuda:1\n",
      "model.layers.17.input_layernorm.weight is on cuda:1\n",
      "model.layers.17.post_attention_layernorm.weight is on cuda:1\n",
      "model.layers.18.self_attn.q_proj.weight is on cuda:1\n",
      "model.layers.18.self_attn.k_proj.weight is on cuda:1\n",
      "model.layers.18.self_attn.v_proj.weight is on cuda:1\n",
      "model.layers.18.self_attn.o_proj.weight is on cuda:1\n",
      "model.layers.18.mlp.gate_proj.weight is on cuda:1\n",
      "model.layers.18.mlp.up_proj.weight is on cuda:1\n",
      "model.layers.18.mlp.down_proj.weight is on cuda:1\n",
      "model.layers.18.input_layernorm.weight is on cuda:1\n",
      "model.layers.18.post_attention_layernorm.weight is on cuda:1\n",
      "model.layers.19.self_attn.q_proj.weight is on cuda:1\n",
      "model.layers.19.self_attn.k_proj.weight is on cuda:1\n",
      "model.layers.19.self_attn.v_proj.weight is on cuda:1\n",
      "model.layers.19.self_attn.o_proj.weight is on cuda:1\n",
      "model.layers.19.mlp.gate_proj.weight is on cuda:1\n",
      "model.layers.19.mlp.up_proj.weight is on cuda:1\n",
      "model.layers.19.mlp.down_proj.weight is on cuda:1\n",
      "model.layers.19.input_layernorm.weight is on cuda:1\n",
      "model.layers.19.post_attention_layernorm.weight is on cuda:1\n",
      "model.layers.20.self_attn.q_proj.weight is on cuda:1\n",
      "model.layers.20.self_attn.k_proj.weight is on cuda:1\n",
      "model.layers.20.self_attn.v_proj.weight is on cuda:1\n",
      "model.layers.20.self_attn.o_proj.weight is on cuda:1\n",
      "model.layers.20.mlp.gate_proj.weight is on cuda:1\n",
      "model.layers.20.mlp.up_proj.weight is on cuda:1\n",
      "model.layers.20.mlp.down_proj.weight is on cuda:1\n",
      "model.layers.20.input_layernorm.weight is on cuda:1\n",
      "model.layers.20.post_attention_layernorm.weight is on cuda:1\n",
      "model.layers.21.self_attn.q_proj.weight is on cuda:1\n",
      "model.layers.21.self_attn.k_proj.weight is on cuda:1\n",
      "model.layers.21.self_attn.v_proj.weight is on cuda:1\n",
      "model.layers.21.self_attn.o_proj.weight is on cuda:1\n",
      "model.layers.21.mlp.gate_proj.weight is on cuda:1\n",
      "model.layers.21.mlp.up_proj.weight is on cuda:1\n",
      "model.layers.21.mlp.down_proj.weight is on cuda:1\n",
      "model.layers.21.input_layernorm.weight is on cuda:1\n",
      "model.layers.21.post_attention_layernorm.weight is on cuda:1\n",
      "model.layers.22.self_attn.q_proj.weight is on cuda:1\n",
      "model.layers.22.self_attn.k_proj.weight is on cuda:1\n",
      "model.layers.22.self_attn.v_proj.weight is on cuda:1\n",
      "model.layers.22.self_attn.o_proj.weight is on cuda:1\n",
      "model.layers.22.mlp.gate_proj.weight is on cuda:1\n",
      "model.layers.22.mlp.up_proj.weight is on cuda:1\n",
      "model.layers.22.mlp.down_proj.weight is on cuda:1\n",
      "model.layers.22.input_layernorm.weight is on cuda:1\n",
      "model.layers.22.post_attention_layernorm.weight is on cuda:1\n",
      "model.layers.23.self_attn.q_proj.weight is on cuda:1\n",
      "model.layers.23.self_attn.k_proj.weight is on cuda:1\n",
      "model.layers.23.self_attn.v_proj.weight is on cuda:1\n",
      "model.layers.23.self_attn.o_proj.weight is on cuda:1\n",
      "model.layers.23.mlp.gate_proj.weight is on cuda:1\n",
      "model.layers.23.mlp.up_proj.weight is on cuda:1\n",
      "model.layers.23.mlp.down_proj.weight is on cuda:1\n",
      "model.layers.23.input_layernorm.weight is on cuda:1\n",
      "model.layers.23.post_attention_layernorm.weight is on cuda:1\n",
      "model.layers.24.self_attn.q_proj.weight is on cuda:1\n",
      "model.layers.24.self_attn.k_proj.weight is on cuda:1\n",
      "model.layers.24.self_attn.v_proj.weight is on cuda:1\n",
      "model.layers.24.self_attn.o_proj.weight is on cuda:1\n",
      "model.layers.24.mlp.gate_proj.weight is on cuda:1\n",
      "model.layers.24.mlp.up_proj.weight is on cuda:1\n",
      "model.layers.24.mlp.down_proj.weight is on cuda:1\n",
      "model.layers.24.input_layernorm.weight is on cuda:1\n",
      "model.layers.24.post_attention_layernorm.weight is on cuda:1\n",
      "model.layers.25.self_attn.q_proj.weight is on cuda:1\n",
      "model.layers.25.self_attn.k_proj.weight is on cuda:1\n",
      "model.layers.25.self_attn.v_proj.weight is on cuda:1\n",
      "model.layers.25.self_attn.o_proj.weight is on cuda:1\n",
      "model.layers.25.mlp.gate_proj.weight is on cuda:1\n",
      "model.layers.25.mlp.up_proj.weight is on cuda:1\n",
      "model.layers.25.mlp.down_proj.weight is on cuda:1\n",
      "model.layers.25.input_layernorm.weight is on cuda:1\n",
      "model.layers.25.post_attention_layernorm.weight is on cuda:1\n",
      "model.layers.26.self_attn.q_proj.weight is on cuda:1\n",
      "model.layers.26.self_attn.k_proj.weight is on cuda:1\n",
      "model.layers.26.self_attn.v_proj.weight is on cuda:1\n",
      "model.layers.26.self_attn.o_proj.weight is on cuda:1\n",
      "model.layers.26.mlp.gate_proj.weight is on cuda:1\n",
      "model.layers.26.mlp.up_proj.weight is on cuda:1\n",
      "model.layers.26.mlp.down_proj.weight is on cuda:1\n",
      "model.layers.26.input_layernorm.weight is on cuda:1\n",
      "model.layers.26.post_attention_layernorm.weight is on cuda:1\n",
      "model.layers.27.self_attn.q_proj.weight is on cuda:1\n",
      "model.layers.27.self_attn.k_proj.weight is on cuda:1\n",
      "model.layers.27.self_attn.v_proj.weight is on cuda:1\n",
      "model.layers.27.self_attn.o_proj.weight is on cuda:1\n",
      "model.layers.27.mlp.gate_proj.weight is on cuda:1\n",
      "model.layers.27.mlp.up_proj.weight is on cuda:1\n",
      "model.layers.27.mlp.down_proj.weight is on cuda:1\n",
      "model.layers.27.input_layernorm.weight is on cuda:1\n",
      "model.layers.27.post_attention_layernorm.weight is on cuda:1\n",
      "model.layers.28.self_attn.q_proj.weight is on cuda:1\n",
      "model.layers.28.self_attn.k_proj.weight is on cuda:1\n",
      "model.layers.28.self_attn.v_proj.weight is on cuda:1\n",
      "model.layers.28.self_attn.o_proj.weight is on cuda:1\n",
      "model.layers.28.mlp.gate_proj.weight is on cuda:1\n",
      "model.layers.28.mlp.up_proj.weight is on cuda:1\n",
      "model.layers.28.mlp.down_proj.weight is on cuda:1\n",
      "model.layers.28.input_layernorm.weight is on cuda:1\n",
      "model.layers.28.post_attention_layernorm.weight is on cuda:1\n",
      "model.layers.29.self_attn.q_proj.weight is on cuda:1\n",
      "model.layers.29.self_attn.k_proj.weight is on cuda:1\n",
      "model.layers.29.self_attn.v_proj.weight is on cuda:1\n",
      "model.layers.29.self_attn.o_proj.weight is on cuda:1\n",
      "model.layers.29.mlp.gate_proj.weight is on cuda:1\n",
      "model.layers.29.mlp.up_proj.weight is on cuda:1\n",
      "model.layers.29.mlp.down_proj.weight is on cuda:1\n",
      "model.layers.29.input_layernorm.weight is on cuda:1\n",
      "model.layers.29.post_attention_layernorm.weight is on cuda:1\n",
      "model.layers.30.self_attn.q_proj.weight is on cuda:1\n",
      "model.layers.30.self_attn.k_proj.weight is on cuda:1\n",
      "model.layers.30.self_attn.v_proj.weight is on cuda:1\n",
      "model.layers.30.self_attn.o_proj.weight is on cuda:1\n",
      "model.layers.30.mlp.gate_proj.weight is on cuda:1\n",
      "model.layers.30.mlp.up_proj.weight is on cuda:1\n",
      "model.layers.30.mlp.down_proj.weight is on cuda:1\n",
      "model.layers.30.input_layernorm.weight is on cuda:1\n",
      "model.layers.30.post_attention_layernorm.weight is on cuda:1\n",
      "model.layers.31.self_attn.q_proj.weight is on cuda:1\n",
      "model.layers.31.self_attn.k_proj.weight is on cuda:1\n",
      "model.layers.31.self_attn.v_proj.weight is on cuda:1\n",
      "model.layers.31.self_attn.o_proj.weight is on cuda:1\n",
      "model.layers.31.mlp.gate_proj.weight is on cuda:1\n",
      "model.layers.31.mlp.up_proj.weight is on cuda:1\n",
      "model.layers.31.mlp.down_proj.weight is on cuda:1\n",
      "model.layers.31.input_layernorm.weight is on cuda:1\n",
      "model.layers.31.post_attention_layernorm.weight is on cuda:1\n",
      "model.norm.weight is on cuda:1\n",
      "lm_head.weight is on cuda:1\n"
     ]
    }
   ],
   "source": [
    "# For model_0\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name} is on {param.device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "\n",
    "\n",
    "system_message = \"\"\"\n",
    "You are an AI assistant designed to answer questions.\n",
    "Please restrict your answer to the exact question and use the exact answer format asked.\n",
    "Answer should not have implied information. If the answer is yes, always provide related phrases. \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def colorize_text(text):\n",
    "    for word, color in zip([\"Reasoning\", \"Question\", \"Answer\", \"Total time\"], [\"blue\", \"red\", \"green\", \"magenta\"]):\n",
    "        text = text.replace(f\"{word}:\", f\"\\n\\n**<font color='{color}'>{word}:</font>**\")\n",
    "    return text\n",
    "\n",
    "def format_prompt(text):\n",
    "    question_content = \"Does the paragraph mention any of the following topics:\\n\"\n",
    "    for i in range(len(query_df)):\n",
    "        question_content += f\"  ({i+1}) {query_df.fea[i]}: {query_df.description[i]}.\\n\"\n",
    "    answer_content = \"Return answer in format:\\n\"\n",
    "    for i in range(len(query_df)): \n",
    "        answer_content += f\"  ({i+1}) {query_df.fea[i]}: [yes/no], related phrases if any: \\n\"\n",
    "    paragragh_content = f\"Paragraph: '{text}' \\n\"\n",
    "    user_message = question_content + answer_content + paragragh_content\n",
    "    #print(user_message)\n",
    "    \n",
    "    return user_message\n",
    "\n",
    "\n",
    "\n",
    "def query_model_batch(\n",
    "        system_message,\n",
    "        user_messages,\n",
    "        temperature=0,\n",
    "        max_length=1024\n",
    "    ):\n",
    "    start_time = time()\n",
    "    # Add \"Question: ... Answer:\" to each user message for clarity\n",
    "    batched_messages = [\n",
    "        \"Question: \" + message + \" Answer:\" for message in user_messages\n",
    "    ]\n",
    "    \n",
    "    # Construct prompts for each message in batch\n",
    "    all_prompts = [\n",
    "        pipe.tokenizer.apply_chat_template(\n",
    "            [\n",
    "                {\"role\": \"system\", \"content\": system_message},\n",
    "                {\"role\": \"user\", \"content\": user_message}\n",
    "            ],\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        ) for user_message in batched_messages\n",
    "    ]\n",
    "    \n",
    "    # Define the end-of-sequence terminators\n",
    "    terminators = [\n",
    "        pipe.tokenizer.eos_token_id,\n",
    "        pipe.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "    ]\n",
    "    \n",
    "    # Run the batch inference\n",
    "    sequences = pipe(\n",
    "        all_prompts,\n",
    "        do_sample=True,\n",
    "        top_p=0.5,\n",
    "        temperature=temperature,\n",
    "        num_return_sequences=1,\n",
    "        eos_token_id=terminators,\n",
    "        max_new_tokens=max_length,\n",
    "        return_full_text=False,\n",
    "        pad_token_id=terminators[0]\n",
    "    )\n",
    "    \n",
    "    # Extract generated text for each sequence\n",
    "    answers = []\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        answer = sequence[0]['generated_text']\n",
    "        total_time = f\"Total time: {round(time() - start_time, 2)} sec.\"\n",
    "        # Format the response with timing information\n",
    "        answers.append(batched_messages[i] + \" \" + answer + \" \" + total_time)\n",
    "\n",
    "    return answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paths\n",
    "- raw data stored under folder \"./data\"\n",
    "- results from llama or gpt should be stored under \"./gpt_data\" and \"./llama_data\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(77175, 2)\n",
      "Does the paragraph mention any of the following topics:\n",
      "  (1) relation: Family and social relationships.\n",
      "  (2) protein: High protein diet, carbohydrate-reduced(low-carb) high-protein diet.\n",
      "  (3) ed: Eating disorders(ED) diagnosis or recovery, ED includes anorexia nervosa, anorexic, bulimia, bulimic, binge eating disorders, arfid, osfed, pica.\n",
      "  (4) exercise: Physical exercise.\n",
      "  (5) meal: Routine of meals.\n",
      "  (6) crave: Craving for high calorie food or carbs.\n",
      "  (7) restrict: Restrict nutrition or calorie intake.\n",
      "  (8) binge: Binge eating.\n",
      "  (9) loss: Body weight loss.\n",
      "  (10) gain: Body weight gain.\n",
      "  (11) calorie: Count calorie.\n",
      "  (12) thinspo: Drive for thinness, want to be thinner or skinny.\n",
      "  (13) leanbody: Drive for lean body mass, low body fat, more muscular or muscles. Must related to body shape..\n",
      "  (14) bodyhate: Body dissatisfaction, feel bad about body image and appearrance. Must related to body image..\n",
      "  (15) feargain: Fear of body weight gain. Must related to body weight.\n",
      "  (16) fearfood: Fear certain foods.\n",
      "  (17) fearcarb: Fear food with high carbohydrate content or sugar..\n",
      "  (18) nosocialeat: Avoidance of social eating, avoidance of eating with other people..\n",
      "  (19) depressedmood: Depressed mood, feeling depressed..\n",
      "Return answer in format:\n",
      "  (1) relation: [yes/no], related phrases if any: \n",
      "  (2) protein: [yes/no], related phrases if any: \n",
      "  (3) ed: [yes/no], related phrases if any: \n",
      "  (4) exercise: [yes/no], related phrases if any: \n",
      "  (5) meal: [yes/no], related phrases if any: \n",
      "  (6) crave: [yes/no], related phrases if any: \n",
      "  (7) restrict: [yes/no], related phrases if any: \n",
      "  (8) binge: [yes/no], related phrases if any: \n",
      "  (9) loss: [yes/no], related phrases if any: \n",
      "  (10) gain: [yes/no], related phrases if any: \n",
      "  (11) calorie: [yes/no], related phrases if any: \n",
      "  (12) thinspo: [yes/no], related phrases if any: \n",
      "  (13) leanbody: [yes/no], related phrases if any: \n",
      "  (14) bodyhate: [yes/no], related phrases if any: \n",
      "  (15) feargain: [yes/no], related phrases if any: \n",
      "  (16) fearfood: [yes/no], related phrases if any: \n",
      "  (17) fearcarb: [yes/no], related phrases if any: \n",
      "  (18) nosocialeat: [yes/no], related phrases if any: \n",
      "  (19) depressedmood: [yes/no], related phrases if any: \n",
      "Paragraph: 'i have always wondered how people with ana make it when theyre eating very few calorie s. do their organs not shut down. i suffered with an eating disorder for a few year s and i am recovered now but i never struggled with severe anorexia. does the body only begin to cannibalize the organs after all of their fat storage is gone. what is the process.' \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "answer_df = pd.read_csv(\"./data/answer_df_raw.csv\")\n",
    "query_df = pd.read_csv(\"./data/fea_df.csv\")\n",
    "print(answer_df.shape)\n",
    "\n",
    "# # test example\n",
    "\n",
    "# text = \"i have always wondered how people with ana make it when theyre eating very few calorie s. do their organs not shut down. i suffered with an eating disorder for a few year s and i am recovered now but i never struggled with severe anorexia. does the body only begin to cannibalize the organs after all of their fat storage is gone. what is the process.\"\n",
    "\n",
    "\n",
    "# prompts = [format_prompt(text)]\n",
    "# responses = query_model_batch(\n",
    "#         system_message=system_message,\n",
    "#         user_messages=prompts,\n",
    "#         temperature=0.5,\n",
    "#         max_length=512\n",
    "#     )\n",
    "\n",
    "\n",
    "# # Display and process responses in a loop\n",
    "# for i, response in enumerate(responses):\n",
    "#     print(colorize_text(f\"{response}\"))\n",
    "\n",
    "print(format_prompt(\"i have always wondered how people with ana make it when theyre eating very few calorie s. do their organs not shut down. i suffered with an eating disorder for a few year s and i am recovered now but i never struggled with severe anorexia. does the body only begin to cannibalize the organs after all of their fat storage is gone. what is the process.\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        sm_id                                         text_w_eos  \\\n",
      "3070  12wvfn8  i have really hit a new low here. i do not kno...   \n",
      "3071  12wwf17  pretty sure this whole thing is a scam because...   \n",
      "3072  12wwm61  i have been overweight ever since i can rememb...   \n",
      "3073  12wxnn0  hi all i hope itsec okay to ask here im conduc...   \n",
      "3074  12wzhzp  what uncommon indian dish did you really enjoy...   \n",
      "3075  12wzoxk  so i love eating avocado and hardboiled egg sa...   \n",
      "3076  12x0okm  hi just an overall trigger warning for everyth...   \n",
      "3077  12x29qn  i got egg white powder on discount and baked 3...   \n",
      "3078  12x2vkx  hi. so my fridge is a chaotic mess and i just ...   \n",
      "3079  12x71qs  get your dunce hats out fittit itsec time for ...   \n",
      "\n",
      "                                          answer_string  \\\n",
      "3070   (1) relation: no, \\n(2) protein: no, \\n(3) th...   \n",
      "3071   (1) relation: no, \\n(2) protein: no, \\n(3) th...   \n",
      "3072   (1) relation: yes, related phrases if any:'my...   \n",
      "3073   (1) relation: no, \\n(2) protein: no, \\n(3) th...   \n",
      "3074   (1) relation: no, \\n(2) protein: no, \\n(3) th...   \n",
      "3075   (1) relation: no, \\n(2) protein: yes, related...   \n",
      "3076   (1) relation: yes, related phrases if any:'mo...   \n",
      "3077   (1) relation: no, \\n(2) protein: yes, related...   \n",
      "3078   (1) relation: no, \\n(2) protein: no, \\n(3) th...   \n",
      "3079   (1) relation: no, \\n(2) protein: no, \\n(3) th...   \n",
      "\n",
      "                                         answer_string2  \\\n",
      "3070   (1) thinspo: yes, related phrases if any: 'i ...   \n",
      "3071   (1) thinspo: no, \\n(2) leanbody: no, \\n(3) bo...   \n",
      "3072   (1) thinspo: yes, related phrases if any: \"wa...   \n",
      "3073   (1) thinspo: no, \\n(2) leanbody: no, \\n(3) bo...   \n",
      "3074   (1) thinspo: no, \\n(2) leanbody: no, \\n(3) bo...   \n",
      "3075   (1) thinspo: no, \\n(2) leanbody: no, \\n(3) bo...   \n",
      "3076   (1) thinspo: yes, related phrases if any: \"Dr...   \n",
      "3077   (1) thinspo: no, \\n(2) leanbody: yes, related...   \n",
      "3078   (1) thinspo: no, \\n(2) leanbody: no, \\n(3) bo...   \n",
      "3079   (1) thinspo: no, \\n(2) leanbody: no, \\n(3) bo...   \n",
      "\n",
      "                                         answer_string3  \n",
      "3070   (1) nosocialeat: yes, related phrases: 'eatin...  \n",
      "3071   (1) nosocialeat: no, related phrases: none\\n(...  \n",
      "3072   (1) nosocialeat: yes, related phrases: 'the c...  \n",
      "3073   (1) nosocialeat: no, related phrases: none\\n(...  \n",
      "3074   (1) nosocialeat: no, related phrases: none\\n(...  \n",
      "3075   (1) nosocialeat: no, related phrases: none\\n(...  \n",
      "3076   (1) nosocialeat: yes, related phrases: 'i try...  \n",
      "3077   (1) nosocialeat: no, related phrases: none\\n(...  \n",
      "3078   (1) nosocialeat: no, related phrases: none\\n(...  \n",
      "3079   (1) nosocialeat: no, related phrases: none\\n(...  \n"
     ]
    }
   ],
   "source": [
    "for k in range(1):\n",
    "    batch_size = 10\n",
    "\n",
    "    # Filter for rows where 'answer_string' is NaN\n",
    "    unanswered_df = answer_df[answer_df['answer_string'].isna()]\n",
    "\n",
    "    # Get the indices of these NaN entries in the original DataFrame\n",
    "    indices_to_update = unanswered_df.index[:batch_size]\n",
    "\n",
    "    # Prepare prompt content for the first 10 entries with NaN answer_string\n",
    "    prompts = [format_prompt(text) for text in unanswered_df['text_w_eos'].iloc[:batch_size]]\n",
    "\n",
    "    # Save the indices list if needed for later use\n",
    "    indices_to_update_list = list(indices_to_update)\n",
    "\n",
    "\n",
    "    # Batch process all prompts at once\n",
    "    responses = query_model_batch(\n",
    "        system_message=system_message,\n",
    "        user_messages=prompts,\n",
    "        temperature=0.1,\n",
    "        max_length=512\n",
    "    )\n",
    "\n",
    "\n",
    "    # Display and process responses in a loop\n",
    "    for i, response in enumerate(responses):\n",
    "        #display(Markdown(colorize_text(f\"{response}\")))\n",
    "\n",
    "        # Extract answer if available\n",
    "        if \"Answer:\" in response:\n",
    "            answer = response.split(\"Answer:\")[1]\n",
    "            # Use the original index from indices_to_update_list\n",
    "            answer_df.loc[indices_to_update_list[i], 'answer_string'] = answer\n",
    "        else:\n",
    "            # Use the original index from indices_to_update_list\n",
    "            answer_df.loc[indices_to_update_list[i], 'answer_string'] = \"Answer not found\"\n",
    "\n",
    "\n",
    "    print(answer_df.loc[indices_to_update_list, :])\n",
    "    answer_df.to_csv('./gpt_data/answer_df.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
