{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9696638-d547-40b3-a68b-74d83c925ebc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5a09acd938045eebb9d37ebdf3cf4bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jq2uw/.local/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/jq2uw/.local/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/jq2uw/.local/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/jq2uw/.local/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "device = 0 if torch.cuda.is_available() else -1  # Use GPU if available, otherwise fallback to CPU\n",
    "model_id = \"lmsys/vicuna-13b-v1.5\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, \n",
    "                                             torch_dtype=torch.bfloat16, \n",
    "                                             device_map=device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b73f1c32-451d-48b3-8279-09f5dc0c884f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_id = 1\n",
    "answer_df_path = \"../../vicuna13b_data/answer_df_part\"+str(file_id)+\".csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5769e88-e313-45ad-9b79-64200632fb34",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4824, 3)\n",
      "        sm_id                                         text_w_eos  \\\n",
      "4280  13wviyb  i first joined to just lose weight but after t...   \n",
      "4281  13wvpld  no one else in my life would understand but to...   \n",
      "4282  13wwakl  currently i enjoy coffemate french vanilla alt...   \n",
      "\n",
      "                                          answer_string  \n",
      "4280  \\n(1) relation: yes, related phrases in the pa...  \n",
      "4281  \\n(1) relation: yes, related phrases in the pa...  \n",
      "4282  \\n(1) relation: yes, related phrases in the pa...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "answer_df = pd.read_csv(answer_df_path)\n",
    "query_df = pd.read_csv(\"../../data/fea_df.csv\")\n",
    "\n",
    "if not 'answer_string' in answer_df.columns:\n",
    "    answer_df['answer_string'] = np.nan\n",
    "    \n",
    "print(answer_df.shape)\n",
    "print(answer_df.iloc[4280:4283,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "650c3c61-1402-46e9-b9fb-3e095c571c68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "system_message = \"\"\"\n",
    "You are an AI assistant designed to answer questions. Restrict your answer to the exact question and use the exact answer format asked. If the answer is yes, always provide related phrases. \n",
    "\"\"\"\n",
    "\n",
    "def format_user_message(text):\n",
    "    question_content = \"Question: Does the paragraph mention any of the following topics:\\n\"\n",
    "    for i in range(len(query_df)):\n",
    "        question_content += f\"  ({i+1}) {query_df.fea[i]}: {query_df.description[i]}.\\n\"\n",
    "    answer_content = \"Return answer in format: Answer:\\n\"\n",
    "    for i in range(len(query_df)): \n",
    "        answer_content += f\"  ({i+1}) {query_df.fea[i]}: yes/no, related phrases in the paragraph if any: \\n\"\n",
    "    paragragh_content = f\"Paragraph: '{text}' \\n\"\n",
    "    user_message = system_message + question_content + paragragh_content + answer_content\n",
    "    #print(user_message)\n",
    "    \n",
    "    return user_message\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a85adf33-f7d6-4583-883e-0d8eab69769a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch:   0%|          | 0/1000 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for k in tqdm(range(1000), desc=\"Processing batch\"):\n",
    "    \n",
    "    batch_size = 10\n",
    "\n",
    "    # Filter for rows where 'answer_string' is NaN\n",
    "    unanswered_df = answer_df[answer_df['answer_string'].isna()]\n",
    "\n",
    "    # Get the indices of these NaN entries in the original DataFrame\n",
    "    indices_to_update = unanswered_df.index[:batch_size]\n",
    "\n",
    "    # Prepare prompt content for the first 10 entries with NaN answer_string\n",
    "    user_messages = [format_user_message(text) for text in unanswered_df['text_w_eos'].iloc[:batch_size]]\n",
    "\n",
    "    # Save the indices list if needed for later use\n",
    "    indices_to_update_list = list(indices_to_update)\n",
    "\n",
    "\n",
    "    if len(indices_to_update_list)>0:\n",
    "        for i in range(len(user_messages)):\n",
    "            failed = True\n",
    "            failed_count = 0\n",
    "            while failed and failed_count<10:\n",
    "                input_ids = tokenizer(user_messages[i], return_tensors='pt').input_ids.to(model.device)\n",
    "                output = model.generate(inputs=input_ids, \n",
    "                                        temperature=0.1,\n",
    "                                        do_sample=True, \n",
    "                                        max_new_tokens=512)\n",
    "                response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "                if (\"Answer:\" in response) and (\"(19) depressedmood:\" in response):\n",
    "                    answer = response.split(\"Answer:\")[-1]\n",
    "                    answer_df.loc[indices_to_update_list[i], 'answer_string'] = answer\n",
    "                    # print(answer)\n",
    "                    failed = False\n",
    "                else:\n",
    "                    failed = True\n",
    "                    failed_count += 1\n",
    "                    # Use the original index from indices_to_update_list\n",
    "                    answer_df.loc[indices_to_update_list[i], 'answer_string'] = \"Answer not found\"\n",
    "\n",
    "        print(answer_df.loc[indices_to_update_list, ['sm_id','answer_string']])\n",
    "        answer_df.to_csv(answer_df_path, index=False)\n",
    "    else:\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f29e48-6084-4117-877d-42fa629e4195",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
