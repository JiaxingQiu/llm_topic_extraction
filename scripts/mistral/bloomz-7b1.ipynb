{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80e98e23-c83d-4543-af5d-8ffdb17fe842",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# pip install -q transformers accelerate\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "checkpoint = \"bigscience/bloomz-7b1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint, \n",
    "                                             torch_dtype=torch.float16, \n",
    "                                             device_map=device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b019214-2a78-42ca-80f5-8c8e40a7f258",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jq2uw/.local/lib/python3.11/site-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translate to English: Je t’aime. I love you.</s>\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer.encode(\"Translate to English: Je t’aime.\", return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(inputs)\n",
    "print(tokenizer.decode(outputs[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1833454-76a0-401b-a67d-9fa16c5c4137",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_id = 1\n",
    "answer_df_path = \"../../bloom_data/answer_df_part\"+str(file_id)+\".csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ddb1f478-adc8-4cbc-b3f0-191956c066cc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9647, 3)\n",
      "     sm_id                                         text_w_eos  answer_string\n",
      "0  10002yt  20 pound down after 22 week s and 17 week s ou...            NaN\n",
      "1  1003i3b  tw ana body dysmorphia describing body potenti...            NaN\n",
      "2  1003xw2  hello fellow brawearing friends. this is going...            NaN\n",
      "3  10042jf  according to cronometer 100 gram of canola oil...            NaN\n",
      "4  1004j21  its been two week s since i started strength t...            NaN\n",
      "5  1008hih  i made panfried chicken thighs ovenroasted pot...            NaN\n",
      "6  100ash9  i have aversive arfid so i have a very bad fea...            NaN\n",
      "7  100aza8  original post here httpswww. reddit. comrcicoc...            NaN\n",
      "8  100bwe6  for context i am early 30s attending a corpora...            NaN\n",
      "9  100c6y2  i got this from a thrift store and was pleasan...            NaN\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "answer_df = pd.read_csv(answer_df_path)\n",
    "query_df = pd.read_csv(\"../../data/fea_df.csv\")\n",
    "\n",
    "if not 'answer_string' in answer_df.columns:\n",
    "    answer_df['answer_string'] = np.nan\n",
    "    \n",
    "print(answer_df.shape)\n",
    "print(answer_df.iloc[0:10,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddaea6c8-20f6-4341-94bb-3183de786c7b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "system_message = \"\"\"\n",
    "You are an AI assistant designed to answer questions.\n",
    "Please restrict your answer to the exact question and use the exact answer format asked.\n",
    "Answer should not have implied information. If the answer is yes, always provide related phrases. \n",
    "\"\"\"\n",
    "\n",
    "def format_user_message(text):\n",
    "    question_content = \"Does the paragraph mention any of the following topics:\\n\"\n",
    "    for i in range(len(query_df)):\n",
    "        question_content += f\"  ({i+1}) {query_df.fea[i]}: {query_df.description[i]}.\\n\"\n",
    "    answer_content = \"Return answer in format:\\n\"\n",
    "    for i in range(len(query_df)): \n",
    "        answer_content += f\"  ({i+1}) {query_df.fea[i]}: [yes/no], related phrases if any: \\n\"\n",
    "    paragragh_content = f\"Paragraph: '{text}' \\n\"\n",
    "    user_message = question_content + answer_content + paragragh_content\n",
    "    #print(user_message)\n",
    "    \n",
    "    return user_message\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7913a71-6abd-4b21-9306-ebc69848096a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch:   0%|          | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# query process\u001b[39;00m\n\u001b[1;32m     18\u001b[0m messages \u001b[38;5;241m=\u001b[39m [ [ {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: system_message}, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: user_message}] \u001b[38;5;28;01mfor\u001b[39;00m user_message \u001b[38;5;129;01min\u001b[39;00m user_messages]\n\u001b[0;32m---> 19\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_chat_template\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_generation_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     23\u001b[0m responses \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(inputs)):\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;66;03m# inputs = tokenizer.encode(\"Translate to English: Je t’aime.\", return_tensors=\"pt\").to(\"cuda\")\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;66;03m# outputs = model.generate(inputs)\u001b[39;00m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;66;03m# print(tokenizer.decode(outputs[0]))\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 19\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# query process\u001b[39;00m\n\u001b[1;32m     18\u001b[0m messages \u001b[38;5;241m=\u001b[39m [ [ {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: system_message}, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: user_message}] \u001b[38;5;28;01mfor\u001b[39;00m user_message \u001b[38;5;129;01min\u001b[39;00m user_messages]\n\u001b[0;32m---> 19\u001b[0m inputs \u001b[38;5;241m=\u001b[39m [\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_chat_template\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_generation_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m messages]\n\u001b[1;32m     23\u001b[0m responses \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(inputs)):\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;66;03m# inputs = tokenizer.encode(\"Translate to English: Je t’aime.\", return_tensors=\"pt\").to(\"cuda\")\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;66;03m# outputs = model.generate(inputs)\u001b[39;00m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;66;03m# print(tokenizer.decode(outputs[0]))\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1803\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.apply_chat_template\u001b[0;34m(self, conversation, tools, documents, chat_template, add_generation_prompt, continue_final_message, tokenize, padding, truncation, max_length, return_tensors, return_dict, return_assistant_tokens_mask, tokenizer_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m   1800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tokenizer_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1801\u001b[0m     tokenizer_kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m-> 1803\u001b[0m chat_template \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_chat_template\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchat_template\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1805\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_assistant_tokens_mask \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m re\u001b[38;5;241m.\u001b[39msearch(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m-?\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms*generation\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms*-?\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m}\u001b[39m\u001b[38;5;124m\"\u001b[39m, chat_template):\n\u001b[1;32m   1806\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning_once(\n\u001b[1;32m   1807\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn_assistant_tokens_mask==True but chat template does not contain `\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;132;01m% g\u001b[39;00m\u001b[38;5;124meneration \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m}` keyword.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1808\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1967\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.get_chat_template\u001b[0;34m(self, chat_template, tools)\u001b[0m\n\u001b[1;32m   1965\u001b[0m         chat_template \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchat_template\n\u001b[1;32m   1966\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1967\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1968\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot use chat template functions because tokenizer.chat_template is not set and no template \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1969\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margument was passed! For information about writing templates and setting the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1970\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizer.chat_template attribute, please see the documentation at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1971\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/docs/transformers/main/en/chat_templating\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1972\u001b[0m         )\n\u001b[1;32m   1974\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m chat_template\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "for k in tqdm(range(1), desc=\"Processing batch\"):\n",
    "    batch_size = 10\n",
    "\n",
    "    # Filter for rows where 'answer_string' is NaN\n",
    "    unanswered_df = answer_df[answer_df['answer_string'].isna()]\n",
    "\n",
    "    # Get the indices of these NaN entries in the original DataFrame\n",
    "    indices_to_update = unanswered_df.index[:batch_size]\n",
    "\n",
    "    # Prepare prompt content for the first 10 entries with NaN answer_string\n",
    "    user_messages = [format_user_message(text) for text in unanswered_df['text_w_eos'].iloc[:batch_size]]\n",
    "\n",
    "    # Save the indices list if needed for later use\n",
    "    indices_to_update_list = list(indices_to_update)\n",
    "\n",
    "    # query process\n",
    "    messages = [ [ {\"role\": \"system\", \"content\": system_message}, {\"role\": \"user\", \"content\": user_message}] for user_message in user_messages]\n",
    "    inputs = [tokenizer.apply_chat_template(\n",
    "        message,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True ) for message in messages]\n",
    "    \n",
    "    \n",
    "    responses = []\n",
    "    for i in range(len(inputs)):\n",
    "        # inputs = tokenizer.encode(\"Translate to English: Je t’aime.\", return_tensors=\"pt\").to(\"cuda\")\n",
    "        # outputs = model.generate(inputs)\n",
    "        # print(tokenizer.decode(outputs[0]))\n",
    "\n",
    "        model_inputs = tokenizer.encode(inputs[i], return_tensors=\"pt\").to(model.device)\n",
    "        outputs = model.generate(model_inputs,\n",
    "                                 max_new_tokens=512,\n",
    "                                 temperature=0.3)\n",
    "        \n",
    "        responses.append(tokenizer.decode(outputs[0], skip_special_tokens=True)[0])\n",
    "    \n",
    "    # Display and process responses in a loop\n",
    "    for i, response in enumerate(responses):\n",
    "        #display(Markdown(colorize_text(f\"{response}\")))\n",
    "        # Extract answer if available\n",
    "        answer = response\n",
    "        # Use the original index from indices_to_update_list\n",
    "        answer_df.loc[indices_to_update_list[i], 'answer_string'] = answer\n",
    "\n",
    "    print(answer_df.loc[indices_to_update_list, ['sm_id','answer_string']])\n",
    "    # answer_df.to_csv(answer_df_path, index=False)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5be55b1-e3ce-42ad-9671-ab51b43283c1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[153772,    427,   9522,   6395,  76721,  68258,     17]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f4e38ea8-08ee-498d-b114-e0990bf61206",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "\n",
    "# Filter for rows where 'answer_string' is NaN\n",
    "unanswered_df = answer_df[answer_df['answer_string'].isna()]\n",
    "\n",
    "# Get the indices of these NaN entries in the original DataFrame\n",
    "indices_to_update = unanswered_df.index[:batch_size]\n",
    "\n",
    "# Prepare prompt content for the first 10 entries with NaN answer_string\n",
    "user_messages = [format_user_message(text) for text in unanswered_df['text_w_eos'].iloc[:batch_size]]\n",
    "\n",
    "# Save the indices list if needed for later use\n",
    "indices_to_update_list = list(indices_to_update)\n",
    "\n",
    "messages = [ system_message+\" \\n \"+user_message for user_message in user_messages]\n",
    " \n",
    "\n",
    "responses = []\n",
    "for i in range(len(messages)):\n",
    "    # inputs = tokenizer.encode(\"Translate to English: Je t’aime.\", return_tensors=\"pt\").to(\"cuda\")\n",
    "    # outputs = model.generate(inputs)\n",
    "    # print(tokenizer.decode(outputs[0]))\n",
    "\n",
    "    inputs = tokenizer.encode(messages[i], return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(inputs,\n",
    "                             max_new_tokens=512,\n",
    "                             do_sample=True,\n",
    "                             temperature=0.3)\n",
    "\n",
    "    responses.append(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "\n",
    "# Display and process responses in a loop\n",
    "for i, response in enumerate(responses):\n",
    "    print(response)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ce756a00-3bdb-4670-9717-1072610387e9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nYou are an AI assistant designed to answer questions.\\nPlease restrict your answer to the exact question and use the exact answer format asked.\\nAnswer should not have implied information. If the answer is yes, always provide related phrases. \\n \\n Does the paragraph mention any of the following topics:\\n  (1) relation: Family and social relationships.\\n  (2) protein: High protein diet, carbohydrate-reduced(low-carb) high-protein diet.\\n  (3) ed: Eating disorders(ED) diagnosis or recovery, ED includes anorexia nervosa, anorexic, bulimia, bulimic, binge eating disorders, arfid, osfed, pica.\\n  (4) exercise: Physical exercise.\\n  (5) meal: Routine of meals.\\n  (6) crave: Craving for high calorie food or carbs.\\n  (7) restrict: Restrict nutrition or calorie intake.\\n  (8) binge: Binge eating.\\n  (9) loss: Body weight loss.\\n  (10) gain: Body weight gain.\\n  (11) calorie: Count calorie.\\n  (12) thinspo: Drive for thinness, want to be thinner or skinny.\\n  (13) leanbody: Drive for lean body mass, low body fat, more muscular or muscles. Must related to body shape..\\n  (14) bodyhate: Body dissatisfaction, feel bad about body image and appearrance. Must related to body image..\\n  (15) feargain: Fear of body weight gain. Must related to body weight.\\n  (16) fearfood: Fear certain foods.\\n  (17) fearcarb: Fear food with high carbohydrate content or sugar..\\n  (18) nosocialeat: Avoidance of social eating, avoidance of eating with other people..\\n  (19) depressedmood: Depressed mood, feeling depressed..\\nReturn answer in format:\\n  (1) relation: [yes/no], related phrases if any: \\n  (2) protein: [yes/no], related phrases if any: \\n  (3) ed: [yes/no], related phrases if any: \\n  (4) exercise: [yes/no], related phrases if any: \\n  (5) meal: [yes/no], related phrases if any: \\n  (6) crave: [yes/no], related phrases if any: \\n  (7) restrict: [yes/no], related phrases if any: \\n  (8) binge: [yes/no], related phrases if any: \\n  (9) loss: [yes/no], related phrases if any: \\n  (10) gain: [yes/no], related phrases if any: \\n  (11) calorie: [yes/no], related phrases if any: \\n  (12) thinspo: [yes/no], related phrases if any: \\n  (13) leanbody: [yes/no], related phrases if any: \\n  (14) bodyhate: [yes/no], related phrases if any: \\n  (15) feargain: [yes/no], related phrases if any: \\n  (16) fearfood: [yes/no], related phrases if any: \\n  (17) fearcarb: [yes/no], related phrases if any: \\n  (18) nosocialeat: [yes/no], related phrases if any: \\n  (19) depressedmood: [yes/no], related phrases if any: \\nParagraph: '20 pound down after 22 week s and 17 week s out from my first wellness show. check the posing progress too starting fat burners and cardio again this week after being sick so excited to see what comes on show day. ' \\nYes\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31cda73-3aaa-4b7c-a9ae-c13709ae5fe2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
